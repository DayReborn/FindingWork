# 25年2月28日

## 手写一个简单版的vector

### 一、Vector的核心功能

- 动态数组，自动扩容
- 随机访问（O(1)时间复杂度）
- 部插入/删除（O(1)均摊时间）
- 支持迭代器
- 深拷贝语义

### 二、代码实现

```c++
#include <iostream>
#include <cassert>

// 步骤 2.1：定义模板类
template <typename T>
class Vector {
private:
    T* data_ = nullptr;       // 数据指针
    size_t size_ = 0;         // 当前元素数量
    size_t capacity_ = 0;     // 当前分配的内存容量

public:
    // 默认构造函数
    Vector() = default;

    // 析构函数
    ~Vector() {
        delete[] data_;
    }


    //==========================================================
    // 步骤 2.2：实现push_back和扩容
    void push_back(const T& value) {
        if (size_ >= capacity_) {
            // 扩容策略：容量为0时设为1，否则翻倍
            size_t new_cap = (capacity_ == 0) ? 1 : capacity_ * 2;
            reserve(new_cap);
        }
        data_[size_++] = value; // 在尾部插入元素
    }
  
    void reserve(size_t new_cap) {
        if (new_cap > capacity_) {
            T* new_data = new T[new_cap]; // 分配新内存
            // 复制旧数据到新内存
            for (size_t i = 0; i < size_; ++i) {
                new_data[i] = data_[i];
            }
            delete[] data_;      // 释放旧内存
            data_ = new_data;
            capacity_ = new_cap;
        }
    }
    //==========================================================


    //==========================================================
    // 步骤 3.1：拷贝构造函数
    Vector(const Vector& other) {
        data_ = new T[other.capacity_];
        size_ = other.size_;
        capacity_ = other.capacity_;
        for (size_t i = 0; i < size_; ++i) {
            data_[i] = other.data_[i]; // 深拷贝元素
        }
    }
  
    Vector& operator=(Vector other) { // 传值调用，直接复用拷贝构造
        swap(other);
        return *this;
    }
  
    // 步骤 3.2：拷贝赋值运算符（使用copy-and-swap惯用法）
    void swap(Vector& other) noexcept {
        std::swap(data_, other.data_);
        std::swap(size_, other.size_);
        std::swap(capacity_, other.capacity_);
    }

    Vector& operator=(Vector other) { // 传值调用，直接复用拷贝构造
        swap(other);
        return *this;
    }
  
    void swap(Vector& other) noexcept {
        std::swap(data_, other.data_);
        std::swap(size_, other.size_);
        std::swap(capacity_, other.capacity_);
    }
    //==========================================================


    //==========================================================
    //步骤 3.3：移动构造函数和移动赋值
    // 移动构造函数
    Vector(Vector&& other) noexcept 
    : data_(other.data_), size_(other.size_), capacity_(other.capacity_) {
    other.data_ = nullptr;
    other.size_ = 0;
    other.capacity_ = 0;
    }

    // 移动赋值运算符
    Vector& operator=(Vector&& other) noexcept {
    if (this != &other) {
        delete[] data_;
        data_ = other.data_;
        size_ = other.size_;
        capacity_ = other.capacity_;
        other.data_ = nullptr;
        other.size_ = 0;
        other.capacity_ = 0;
    }
    return *this;
    }
    //==========================================================


    //==========================================================
    //4. 完善元素访问与迭代器
    //步骤 4.1：operator[]和at()
    T& operator[](size_t index) {
        return data_[index];
    }
  
    const T& operator[](size_t index) const {
        return data_[index];
    }
  
    T& at(size_t index) {
        if (index >= size_) {
            throw std::out_of_range("Index out of range");
        }
        return data_[index];
    }
    //==========================================================


    //==========================================================
    //步骤 4.2：迭代器支持
    using iterator = T*;
    using const_iterator = const T*;

    iterator begin() { return data_; }
    iterator end() { return data_ + size_; }
    const_iterator begin() const { return data_; }
    const_iterator end() const { return data_ + size_; }
    //==========================================================

    //==========================================================
    // 5. 高级操作：insert、erase和emplace_back
    // 步骤 5.1：insert实现
    iterator insert(iterator pos, const T& value) {
        size_t offset = pos - begin();
        if (size_ >= capacity_) {
            reserve(capacity_ == 0 ? 1 : capacity_ * 2);
        }
        pos = begin() + offset; // 防止扩容后迭代器失效
        // 将pos后的元素后移
        for (auto it = end(); it > pos; --it) {
            *it = std::move(*(it - 1));
        }
        *pos = value;
        ++size_;
        return pos;
    }

    //步骤 5.2：erase实现
    iterator erase(iterator pos) {
        if (pos < begin() || pos >= end()) {
            throw std::out_of_range("Invalid iterator");
        }
        // 将pos后的元素前移
        std::move(pos + 1, end(), pos);
        --size_;
        return pos;
    }

    //步骤 5.3：emplace_back（完美转发）
    template <typename... Args>
    void emplace_back(Args&&... args) {
        if (size_ >= capacity_) {
            reserve(capacity_ == 0 ? 1 : capacity_ * 2);
        }
        // 在尾部直接构造对象
        new(data_ + size_) T(std::forward<Args>(args)...);
        ++size_;
    }
    //==========================================================
    // 6. 边界处理与优化
    // 步骤 6.1：pop_back和clear
    void pop_back() {
        if (size_ > 0) {
            --size_;
            data_[size_].~T(); // 显式调用析构函数（对非POD类型必要）
        }
    }
  
    void clear() {
        size_ = 0;
    }

    // 步骤 6.2：resize实现
    void resize(size_t new_size, const T& value = T()) {
        if (new_size > size_) {
            reserve(new_size);
            for (size_t i = size_; i < new_size; ++i) {
                data_[i] = value;
            }
        }
        size_ = new_size;
    }
    //==========================================================


    //==========================================================
    // 基础功能占位
    size_t size() const { return size_; }
    size_t capacity() const { return capacity_; }
    bool empty() const { return size_ == 0; }
};


// 步骤 2.3：测试基础功能
void test1_base(){
    Vector<int> v;
    v.push_back(1);
    v.push_back(2);
    assert(v.size() == 2);
    assert(v.capacity() == 2);
}

void test2_up(){
    Vector<int> v;
    v.push_back(1);
    v.emplace_back(2);
    assert(v.size() == 2);

    Vector<int> v2 = v; // 测试拷贝构造
    assert(v2[0] == 1);

    v.insert(v.begin(), 0);
    assert(v[0] == 0);

    v.erase(v.begin());
    assert(v[0] == 1);
}

int main(){
    test1_base();
    // test2_up();
    return 0;
}
```

# 25年2月29日

## 网络IO与IO多路复用

```cpp
#include <iostream>
int main()
{
    return 0;
}
```

首先思考这个代码是怎么被执行的

### 一、网络IO

这边补充说一下不同地址的问题

```
(base) zzx@zzx:~/Finding_work/2.1.1-multi-io$ ifconfig
eno1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.1.122  netmask 255.255.255.0  broadcast 192.168.1.255
        inet6 fe80::82e8:2cff:fe29:ef0e  prefixlen 64  scopeid 0x20<link>
        ether 80:e8:2c:29:ef:0e  txqueuelen 1000  (以太网)
        RX packets 90954  bytes 134647431 (134.6 MB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 43593  bytes 3238996 (3.2 MB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (本地环回)
        RX packets 263  bytes 25795 (25.7 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 263  bytes 25795 (25.7 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

wlo1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.1.116  netmask 255.255.255.0  broadcast 192.168.1.255
        inet6 fe80::d16d:f3a0:7b8f:6ede  prefixlen 64  scopeid 0x20<link>
        ether d4:d2:52:a8:61:19  txqueuelen 1000  (以太网)
        RX packets 1354  bytes 103934 (103.9 KB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 330  bytes 33241 (33.2 KB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

```

> 这边eno1是我们跟外部设备通信的地址，例如我现在用ssh连接。
>
> lo：回环地址则是内部线程之间通信使用到的地址，
>
> any 0.0.0.0：这个则是我们不知道用什么就写这个

### 二、初始化写一个TCP连接

```c
#include <sys/socket.h>     // 套接套接字操作相关函数和数据结构
#include <errno.h>          // 错误码定义（如errno）
#include <netinet/in.h>     // IPv4地址结构sockaddr_in定义

#include <stdio.h>
#include <string.h>
#include <unistd.h>


// tcp
int main()
{
    int sockfd = socket(AF_INET, SOCK_STREAM, 0);   
    struct sockaddr_in serveraddr;

    // 将serveraddr结构体的所有字节置为0，避免残留数据干扰后续操作。
    // 网络编程中结构体可能存在填充字节，未清零可能导致bind()失败
    memset(&serveraddr, 0, sizeof(struct sockaddr_in));
  
    serveraddr.sin_family = AF_INET;
    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);
    serveraddr.sin_port = htons(2048);

    // bind()_绑定套接字与地址。将套接字关联到特定的IP地址和端口，使客户端能通过该地址访问服务。
    if (-1 == bind(sockfd, (struct sockaddr *)&serveraddr, sizeof(struct sockaddr)))
    {
        perror("bind");
        return -1;
    }

    // 将套接字设置为被动模式，准备接受客户端的连接请求。
    listen(sockfd, 10);
    getchar();
}

```

具体的代码解析如下：

**1.`socket()` - 创建通信端点**

```c
int sockfd = socket(AF_INET, SOCK_STREAM, 0);
```

**作用**：创建一个用于网络通信的套接字（类似文件描述符），是后续所有操作的基础。

**参数详解**

- **`AF_INET`**：指定使用IPv4协议族（Address Family）。其他常见值如 `AF_INET6`（IPv6）、`AF_UNIX`（本地通信）。
- **`SOCK_STREAM`**：指定使用面向连接的TCP协议。特点是有序、可靠、双向字节流。若使用UDP则为 `SOCK_DGRAM`。
- **`0`**：自动选择协议类型。对于TCP，等价于 `IPPROTO_TCP`；UDP则为 `IPPROTO_UDP`。

**返回值**

- **成功**：返回非负整数，即套接字描述符（类似文件句柄）。
- **失败**：返回-1，错误码存储在 `errno`中（可通过 `perror`打印）。

**类比**：就像安装了一部电话机，但尚未插线（未绑定地址）或开机（未监听）。

**2. `memset()` - 内存初始化**

```c
memset(&serveraddr, 0, sizeof(struct sockaddr_in));
```

**作用**：将 `serveraddr`结构体的所有字节置为0，避免残留数据干扰后续操作。

**关键性**：网络编程中结构体可能存在填充字节，未清零可能导致 `bind()`失败。

**参数**

- **目标地址**：`&serveraddr`。
- **填充值**：`0`（全部字节置0）。
- **长度**：`sizeof(struct sockaddr_in)`（确保覆盖整个结构体）。

**3. `bind()` - 绑定套接字与地址**

```c
bind(sockfd, (struct sockaddr *)&serveraddr, sizeof(struct sockaddr));
```

**作用**：将套接字关联到特定的IP地址和端口，使客户端能通过该地址访问服务。

**参数解析**

- **`sockfd`**：由 `socket()`返回的描述符。
- **`(struct sockaddr *)&serveraddr`**：将 `sockaddr_in`强制转换为通用地址结构体 `sockaddr`（历史原因，需统一接口）。
- **`sizeof(struct sockaddr)`**：地址结构体的大小。此处有潜在问题（应使用 `sizeof(serveraddr)`）。

**地址结构体字段**

- **`sin_family`**：必须与 `socket()`的地址族一致（此处为 `AF_INET`）。
- **`sin_addr.s_addr`**：IP地址（`INADDR_ANY`表示监听所有本地网卡）。
- **`sin_port`**：端口号（需用 `htons()`转换为网络字节序）。

**常见错误**

- **`EADDRINUSE`**：端口被占用。
- **`EACCES`**：绑定到特权端口（<1024）无root权限。

**类比**：为电话机分配电话号码（IP+端口），允许他人拨打。

**4. `listen()` - 开启监听模式**

```c
listen(sockfd, 10);
```

**作用**：将套接字设置为被动模式，准备接受客户端的连接请求。

**参数详解**

- **`sockfd`**：已绑定地址的套接字。
- **`10`**：内核维护的未完成连接队列的最大长度（实际值可能被系统调整，如Linux默认为128）。

**队列类型**

- **未完成队列（SYN队列）**：客户端发送SYN后处于 `SYN_RCVD`状态。
- **已完成队列（ACCEPT队列）**：已完成三次握手，等待 `accept()`取出。

**返回值**：成功返回0，失败返回-1（需检查错误）。

**类比**：电话机开机并设置为响铃模式，等待来电。

**5. `getchar()` - 阻塞程序退出**

```c
getchar(); // 等待用户输入
```

**作用**：防止程序立即结束，保持服务器运行以便测试。

**实际应用**：真实服务器应使用事件循环（如 `while(1)` + `accept()`）持续处理请求。

**潜在问题**：此处仅为演示，无实际连接处理逻辑。

**6. `perror()` - 打印错误信息**

```c
perror("bind"); // 输出错误原因
```

**作用**：根据 `errno`的值，输出描述性错误信息到标准错误流。

**格式**：输出“bind: 具体错误”，例如“bind: Address already in use”。

**依赖**：需包含 `errno.h`，且函数调用失败后立即使用（`errno`可能被后续调用覆盖）。

**7.函数调用关系图**

```plaintext
socket() → bind() → listen() → getchar()
           |           |
           ↓           ↓
        分配地址      开启监听
```

**8. 关键注意事项总结**

**字节序转换**：

- `htons()`：将16位端口号从主机字节序（小端/大端）转为网络字节序（大端）。
- `htonl()`：用于32位IP地址（如 `INADDR_ANY`的转换）。

**结构体类型转换**：

- `sockaddr_in`（IPv4专用）→ `sockaddr`（通用地址结构），保证接口统一。

### 三、补充接受代码

```
	// 补充接受数据部分
    struct sockaddr_in clientaddr;
    socklen_t len = sizeof(clientaddr);
    //当我们运行代码的时候首先会阻塞在这个地方，而不是阻塞在下面的getchar();
    int clientfd = accept(sockfd, (struct sockaddr *)&clientaddr, &len);

    printf("sockfd:%d\nclientfd:%d", sockfd, clientfd);
    close(clientfd);
```

 **1.`struct sockaddr_in clientaddr`**

- **作用**：定义一个IPv4地址结构体，用于存储**客户端**的连接信息（IP和端口）。
- 关键字段：
  - `sin_family`：地址族（自动填充为 `AF_INET`）。
  - `sin_port`：客户端的端口号（网络字节序）。
  - `sin_addr`：客户端的IP地址（可通过 `inet_ntoa()`转换为字符串）。

---

**2. `socklen_t len = sizeof(clientaddr)`**

- **作用**：声明一个变量表示地址结构体的大小，供 `accept()`函数使用。
- **传递方式**：必须传递指针（`&len`），因为 `accept()`可能修改此值以反映实际写入的地址大小。

---

**3. `accept()` - 接受客户端连接**

```c
int clientfd = accept(sockfd, (struct sockaddr *)&clientaddr, &len);
```

- **作用**：从监听队列中取出一个已完成的连接，并返回一个新的套接字描述符（`clientfd`）。
- 

  参数详解：

- **`sockfd`**：监听套接字（由 `socket()`创建并通过 `bind()`+`listen()`启用）。
- **`(struct sockaddr *)&clientaddr`**：存储客户端地址信息的结构体（需强制类型转换）。
- **`&len`**：输入时为地址结构体大小，输出时为实际写入的地址大小。
- 

  返回值：

- **成功**：返回一个新的套接字描述符（`clientfd`），用于与客户端通信。
- **失败**：返回-1（需检查 `errno`）。
- **阻塞机制**：若监听队列中没有已完成的连接，`accept()`会阻塞直到有客户端连接。
- **类比**：电话接线员从等待队列中接听一个来电，并为该通话分配专用线路（`clientfd`）。

---

**4. `printf()` - 打印文件描述符**

```c
printf("sockfd:%d\nclientfd:%d", sockfd, clientfd);
```

- **输出示例**：`sockfd:3` 和 `clientfd:4`。

  原因分析：

  - **文件描述符分配规则**：Linux内核按最小可用原则分配描述符。
  - 标准流占用：程序启动时自动打开三个文件描述符：
    - `0`：标准输入（stdin）
    - `1`：标准输出（stdout）
    - `2`：标准错误（stderr）
  - 套接字顺序：
    1. `socket()`创建监听套接字时，分配下一个可用的最小描述符（即3 → `sockfd=3`）。
    2. `accept()`创建连接套接字时，继续分配下一个可用值（即4 → `clientfd=4`）

### 四、具体来写接受部分代码

```c
  printf("accept\n");

  char buffer[128] = {0};

  int count = recv(clientfd, buffer, 128, 0);
```

**具体实现的时候我们会使用while来进行嵌套循环检测接受信息，但是有没有办法让我们直接写个接口，检测到就拉起处理呢?**

==有！！！！==，这边可以使用sigio，它会调用sig.handler(sigio, handler)然后拉起我们需要的实现的流程，raise起来。

ps：这个技术很老了  现在一般没有人去使用这个技术了。！！

==思考==

==这边为什么会出现我点击创建服务器，创建tcp连接发送后，关闭代码，但是出现timeout的问题，同时报错bind: Address already in use==

**问题分析：TCP服务器开发中的两个关键问题**

**问题一：`bind: Address already in use` 错误**

**问题二：`recv` 函数导致的超时**

---

**1. `recv` 函数深度解析**

```c
int count = recv(clientfd, buffer, 128, 0);
```

**函数作用**

- 从已连接的套接字（`clientfd`）接收数据，将数据存入 `buffer`。
- **类比**：类似于从水管（连接）中读取水流（数据）。

---

**参数详解**

|     参数     |                                  说明                                  |
| :----------: | :---------------------------------------------------------------------: |
| `clientfd` |               已连接的套接字描述符（由 `accept` 返回）               |
|  `buffer`  |                          存储接收数据的缓冲区                          |
|   `128`   |                       缓冲区最大容量（避免溢出）                       |
|    `0`    | 标志位（常见选项：`MSG_WAITALL`阻塞收满数据，`MSG_DONTWAIT`非阻塞） |

---

**返回值意义**

|      返回值      |                   含义                   |
| :--------------: | :---------------------------------------: |
| **`>0`** | 实际接收的字节数（可能小于请求的128字节） |
| **`0`** |   对方已关闭连接（需关闭 `clientfd`）   |
| **`-1`** |         出错（需检查 `errno`）         |

---

**2. 问题一：`bind: Address already in use` 的根源**

**原因分析**

- ==**TCP 四次挥手机制**：服务器关闭后，端口进入 **`TIME_WAIT` 状态**（默认等待2MSL，约60秒）。==
- ==**直接重启服务器**：试图绑定同一端口时，内核拒绝重用处于 `TIME_WAIT`的地址。==

---

**解决方案**

通过设置 `SO_REUSEADDR` 选项允许地址重用：

```c
int opt = 1;
setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));
```

**代码位置**：在 `bind()` 之前调用。

---

**3. 问题二：`recv` 超时和代码卡死**

**原因分析**

- **阻塞模式默认行为**：`recv` 会无限等待数据到来。
- **客户端未发送数据**：若客户端连接后不发送数据，`recv` 永久阻塞。
- ==**未处理连接关闭**：客户端意外断开时，服务器可能卡在 `recv`。==

---

**解决方案**

**(1) 设置接收超时**

通过 `setsockopt` 设置超时时间：

```c
struct timeval tv;
tv.tv_sec = 5;  // 5秒超时
tv.tv_usec = 0;
setsockopt(clientfd, SOL_SOCKET, SO_RCVTIMEO, &tv, sizeof(tv));
```

**(2) 正确处理返回值**

```c
int count = recv(clientfd, buffer, 128, 0);
if (count > 0) {
    // 成功接收数据
} else if (count == 0) {
    // 客户端正常关闭连接
    close(clientfd);
} else {
    // 处理错误（如超时、连接重置）
    if (errno == EAGAIN || errno == EWOULDBLOCK) {
        printf("Recv timeout!\n");
    } else {
        perror("Recv error");
    }
    close(clientfd);
}
```

### 五、服务端Socket API核心解析

你的老师提到的“服务端只有这些API”，指的是用C语言开发TCP服务端必须掌握的基础系统调用。这些API构成了服务端网络通信的核心骨架，所有服务端程序都基于这些函数构建。以下是详细解析：

---

**服务端开发必备API清单**

|   API名称   |          作用          | 调用顺序 |
| :----------: | :--------------------: | :------: |
| `socket()` | 创建通信端点（套接字） |    1    |
|  `bind()`  |  绑定IP和端口到套接字  |    2    |
| `listen()` |  将套接字设为监听模式  |    3    |
| `accept()` |     接受客户端连接     |    4    |
|  `recv()`  |        接收数据        |    5    |
|  `send()`  |        发送数据        |    6    |
| `close()` |       关闭套接字       |    7    |

### **六、TCP连接的核心状态**

**TCP协议状态机** 中的关键状态，描述了TCP连接从建立到关闭的全生命周期。理解这些状态对网络编程、服务端调试、性能优化至关重要。以下是深度解析：

#### 1. 状态流转图

用一张图快速理解TCP状态流转：

```
客户端                                      服务端
  |                                          |
  |--------------- SYN -------------------->| 
  |<------------- SYN+ACK ------------------| 
  |--------------- ACK -------------------->| 
  |       (ESTABLISHED)               (ESTABLISHED)
  |                                          |
  |--------------- FIN -------------------->| 
  |<------------- ACK ----------------------| 
  |<------------- FIN ----------------------| 
  |--------------- ACK -------------------->| 
  |      (TIME_WAIT)                   (CLOSED)
```

---

#### **2. 详解每个状态**

**1. `LISTEN`（监听状态）**

- **触发条件**：服务端调用 `listen()` 后进入该状态。
- 技术含义：

  - 服务端准备好接收客户端的 `SYN` 连接请求。
  - 操作系统内核维护一个 **未完成连接队列**（存放收到SYN但未完成三次握手的连接）。
- 代码示例：

  ```c
  int sockfd = socket(AF_INET, SOCK_STREAM, 0);
  bind(sockfd, ...);
  listen(sockfd, 5);  // 进入LISTEN状态，队列最大长度为5
  ```
- 关键问题：

  - **队列溢出**：若客户端大量快速发起连接，可能导致队列满，新连接被丢弃（客户端收到 `ECONNREFUSED`）。

**2. `ESTABLISHED`（已建立连接）**

- **触发条件**：完成三次握手后，客户端和服务端均进入此状态。
- 技术含义：
  - 双方可以自由收发数据。
  - 此状态持续到某一方主动发起关闭（调用 `close()` 或发送 `FIN` 包）。
- **现实类比**：电话接通后，双方可以正常对话。

**3. `CLOSE_WAIT`（等待关闭）**

- **触发条件**：服务端收到客户端的 `FIN` 包（即客户端主动关闭），并回复 `ACK` 后进入此状态。
- 技术含义：

  - 服务端需要处理完剩余数据，再调用 `close()` 发送自己的 `FIN` 包。
  - **危险信号**：大量 `CLOSE_WAIT` 状态连接表明服务端未正确关闭套接字（代码bug）。
- 代码问题示例：

  ```c
  // 错误！收到FIN后没有调用close()
  char buffer[1024];
  recv(sockfd, buffer, sizeof(buffer), 0); // 收到0字节（FIN）
  // 忘记 close(sockfd); → 连接永远处于CLOSE_WAIT
  ```
- 解决方案：

  - 确保在代码逻辑中检测到 `recv()` 返回0时，立即关闭套接字。

**4. `TIME_WAIT`（等待计时）**

- **触发条件**：主动关闭方（先发送 `FIN` 的一方）在发送最后一个 `ACK` 后进入此状态。
- 技术含义：

  - 持续 **2MSL**（Maximum Segment Lifetime，通常为60秒）。
  - 确保网络中残留的旧数据包消失，避免影响新连接。
- 现实意义：

  - 想象挂断电话后等待几秒再离开，防止对方还有话没说完。
- 关键问题：

  - **端口占用**：短时间内大量 `TIME_WAIT` 连接可能导致端口耗尽。
  - 解决方案：

    ```c
    int opt = 1;
    setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt)); // 允许重用端口
    ```

#### **3. 其他重要状态**

**1. `SYN_SENT`（同步已发送）**

- **触发条件**：客户端调用 `connect()` 发送 `SYN` 后进入此状态。
- **技术含义**：等待服务端的 `SYN+ACK` 响应。

**2. `SYN_RECEIVED`（同步已接收）**

- **触发条件**：服务端收到 `SYN` 并发送 `SYN+ACK` 后进入此状态。
- **技术含义**：等待客户端的 `ACK` 完成三次握手。

**3. `FIN_WAIT_1` 和 `FIN_WAIT_2`**

- **触发条件**：主动关闭方发送 `FIN` 后进入 `FIN_WAIT_1`，收到 `ACK` 后进入 `FIN_WAIT_2`。
- **技术含义**：等待对方的 `FIN` 包。

#### **4. 实战调试技巧**

**1. 查看TCP状态（Linux）**

```bash
netstat -antp | grep -E 'LISTEN|ESTAB|CLOSE_WAIT|TIME_WAIT'
```

输出示例：

```
Proto Recv-Q Send-Q Local Address   Foreign Address  State       PID/Program
tcp   0      0      0.0.0.0:8080    0.0.0.0:*        LISTEN      1234/server
tcp   0      0      192.168.1.5:8080 10.0.0.3:55678   ESTABLISHED 1234/server
tcp   0      0      192.168.1.5:8080 10.0.0.3:55679   CLOSE_WAIT  1234/server
```

**2. 常见问题诊断**

|        状态        |           可能问题           |                       解决方案                       |
| :-----------------: | :---------------------------: | :---------------------------------------------------: |
| 大量 `CLOSE_WAIT` |     服务端未正确关闭连接     | 检查代码逻辑，确保 `recv() == 0` 时调用 `close()` |
| 大量 `TIME_WAIT` |      短连接频繁创建销毁      |            启用 `SO_REUSEADDR` 或连接池            |
|  大量 `SYN_RECV`  | SYN洪水攻击或服务端处理不过来 |      增加 `listen()` 队列长度，启用SYN Cookie      |

---

#### **5. 总结图表**

|       状态       |    角色    |    触发动作    |   生命周期阶段   |
| :--------------: | :--------: | :-------------: | :--------------: |
|    `LISTEN`    |   服务端   |  `listen()`  |     等待连接     |
|   `SYN_SENT`   |   客户端   |  `connect()`  |     发起连接     |
| `SYN_RECEIVED` |   服务端   |     收到SYN     |    半连接队列    |
| `ESTABLISHED` |    双方    |  完成三次握手  |     数据传输     |
|  `CLOSE_WAIT`  | 被动关闭方 |     收到FIN     |  等待应用层关闭  |
|  `TIME_WAIT`  | 主动关闭方 | 发送最后一个ACK | 等待残留数据消失 |

理解这些状态，你就能像“网络医生”一样精准诊断连接问题！

### 七、添加关闭相应

客户端关闭之后，会提示接收到一个长度为0的数据！！

![image-20250302154759373](进阶学习/image-20250302154759373.png)

添加一个结束代码

```c
char buffer[128] = {0};
int count = recv(clientfd, buffer, 128, 0);
if(count == 0){
    close(clientfd);
    break;
}
send(clientfd, buffer, count, 0);
printf("sockfd:%d\nclientfd:%d\ncount:%d\nbuffer:%s\n", sockfd, clientfd, count, buffer);
```

这也可以解决问题首先就是一直挑不出循环的问题，一直处于这个循环问题的时候准确来说是因为会有一直给进关闭的要求的问题，接受其实就是一个长度为0的请求，所以会一直循环，如果没有 `close(clientfd);`这样的操作的话，整个连接就会处于close_wait 状态，等待关闭这个嵌套字。









## 处理一个服务端对应多个客户端的问题

### 一、多线程处理这个问题

#### 1. 代码实现

```c
void *client_thread(void *arg){
    int clientfd = *(int *)arg;
    while(1){
        char buffer[128] = {0};
        int count = recv(clientfd, buffer, 128, 0);
        if(count == 0){
            break;
        }
        send(clientfd, buffer, count, 0);
        printf("clientfd:%d\ncount:%d\nbuffer:%s\n", clientfd, count, buffer);
    }
    close(clientfd);
    return NULL;
}




#else
    while(1){
        struct sockaddr_in clientaddr;
        socklen_t len = sizeof(clientaddr);
        int clientfd = accept(sockfd, (struct sockaddr *)&clientaddr, &len);
    
        pthread_t thid;
        pthread_create(&thid, NULL, client_thread, &clientfd);
    }

#endif

    getchar();
    // 不要将关闭交给服务端，否则另一边端口不会释放，会一直报Timeout
    close(sockfd);
```

这是经典的**“一连接一线程”（Thread-per-Connection）模型**，适用于中小规模并发场景。

在您提供的代码中，为每个客户端连接创建线程的操作是通过以下两行实现的：

c

```c
pthread_t thid;
pthread_create(&thid, NULL, client_thread, &clientfd);
```

这里涉及多线程编程的核心操作，但存在几个关键问题需要注意。下面详细解释这一过程及其潜在风险：

---

**1. 线程创建的基本流程**

- **`pthread_t thid`**：声明一个线程标识符变量，用于存储新线程的ID。
- **

  ```
  pthread_create
  ```

  **：实际创建线程的函数，其参数依次为：

  - **`&thid`**：线程ID的地址，创建成功后会将新线程的ID写入此处。
  - **`NULL`**：线程属性（如栈大小、调度策略等），此处使用默认值。
  - **`client_thread`**：线程入口函数，新线程启动后执行此函数。
  - **`&clientfd`**：传递给线程函数的参数，此处传递客户端文件描述符的地址。

---

#### **2. 参数传递的致命问题**

代码中直接传递了栈变量 `clientfd` 的地址（`&clientfd`），这会导致以下问题：

**2.1 数据竞争（Data Race）**

- 场景示例：
  - 主线程在 `while(1)` 循环中不断接受新连接，每次循环会覆盖 `clientfd` 的值。
  - 当线程A刚读取 `clientfd` 的值时，主线程可能已进入下一轮循环，修改了 `clientfd`，导致线程A实际处理的是另一个客户端的文件描述符。

**2.2 生命周期不匹配**

- **栈变量的短暂性**：`clientfd` 是主线程栈上的变量，其内存在循环迭代中会被重复使用。如果新线程在读取 `clientfd` 时主线程已进入下一轮循环，线程可能读取到无效或错误的值。

---

#### **3. 正确做法：动态内存分配**

为避免数据竞争和生命周期问题，应为每个线程分配独立的内存来传递参数：

- **分配内存**

```c
int *clientfd_ptr = malloc(sizeof(int));  // 动态分配内存
*clientfd_ptr = clientfd;                // 复制文件描述符的值
```

- **传递内存地址**

```c
pthread_create(&thid, NULL, client_thread, clientfd_ptr);
```

- **线程内释放内存**

在线程函数中释放内存，避免泄漏：

```c
void *client_thread(void *arg) {
    int clientfd = *(int *)arg;
    free(arg);  // 释放动态分配的内存
    // ...处理逻辑
    return NULL;
}
```

#### **4. 线程资源管理**

**4.1 分离线程（Detach）**

默认情况下，线程结束后会保留状态直到被 `pthread_join` 回收（类似进程的僵尸状态）。为避免资源泄漏，应分离线程：

```c
pthread_create(&thid, NULL, client_thread, clientfd_ptr);
pthread_detach(thid);  // 分离线程
```

**4.2 错误处理**

检查 `pthread_create` 的返回值，处理线程创建失败的情况：

```c
if (pthread_create(&thid, NULL, client_thread, clientfd_ptr) != 0) {
    perror("pthread_create failed");
    free(clientfd_ptr);  // 释放内存
    close(clientfd);     // 关闭文件描述符
}
```

---

#### **5. 完整修复后的代码片段**

```c
while(1) {
    struct sockaddr_in clientaddr;
    socklen_t len = sizeof(clientaddr);
    int clientfd = accept(sockfd, (struct sockaddr *)&clientaddr, &len);
    if (clientfd == -1) {
        perror("accept");
        continue;
    }

    // 动态分配内存传递clientfd
    int *clientfd_ptr = malloc(sizeof(int));
    *clientfd_ptr = clientfd;

    pthread_t thid;
    if (pthread_create(&thid, NULL, client_thread, clientfd_ptr) != 0) {
        perror("pthread_create failed");
        free(clientfd_ptr);
        close(clientfd);
    } else {
        pthread_detach(thid);  // 分离线程
    }
}
```

---

#### **6. 总结**

- **为何动态分配**：确保每个线程拥有独立的参数副本，避免主线程循环覆盖数据。
- **为何分离线程**：防止线程结束后资源无法回收，导致内存泄漏。
- **为何检查返回值**：及时处理系统资源不足等异常情况，增强程序健壮性。

通过以上改进，代码能够安全地处理多客户端并发连接，避免数据竞争和资源泄漏问题。





### 二、使用select、poll、epoll

#### 一、不使用多线程

首先我们要分析为什么不使用多线程的方法。

> 创建多线程的话，假设一个线程消耗8M内存，有1w个客户端同时进行连接，我们消耗的缓冲区就已经是80G了，这显然是不可以接受的!

那我们可以使用一个线程的方式来解决问题吗？

==可以！使用select、poll、epoll！==

#### 二、使用select方法

注意select方法仅适用于兼容性要求高的场景。

> select():
>
> 1. connect, select返回

> select()的缺点是什么：
>
> 1. 参数量五个很多
> 2. 每次都需要把待检测的io集合rset完全拷贝进内核
> 3. 对io的数量是有限制的。

> 性能缺陷分析：
>
> 1. io集合rset完全拷贝
> 2. io需要从头到尾遍历

代码实现：

```c
#else
    // @param maxfd: 设置内部循环的最大fd编号
    // @param rset: fd_set 本质是 1024 位的位图，用来标志这个io是否可读
    // @param wset: fd_set，用来标志这个io是否可写
    // @param eset: fd_set，用来标志这个io是否可读
    // @param timeout: 超时控制
    // *int nready = select(maxfd, rset, wset,eset,timeout)这个函数需要五个参数

    fd_set rfds, rset;
    FD_ZERO(&rfds);
    FD_SET(sockfd, &rfds);

    int maxfd = sockfd;

    printf("loop\n");
    while (1)
    {
        rset = rfds;
        int nready = select(maxfd + 1, &rset, NULL, NULL, NULL);
        if (FD_ISSET(sockfd, &rset))
        {
            struct sockaddr_in clientaddr;
            socklen_t len = sizeof(clientaddr);
            int clientfd = accept(sockfd, (struct sockaddr *)&clientaddr, &len);

            printf("sockfd: %d\n", clientfd);

            FD_SET(clientfd, &rfds);
            maxfd = clientfd;
        }
    }
    int i = 0;
    for (i = sockfd + 1; i <= maxfd; ++i)
    {
        if (FD_ISSET(i, &rset))
        {
            char buffer[128] = {0};
            int count = recv(i, buffer, 128, 0);
            if (count == 0)
            {
                printf("disconnect\n");
                // !不关闭事件会一直循环循环
                // close(i);
                FD_CLR(i, &rfds);
                close(i);

                break;
            }
            send(i, buffer, count, 0);
            printf("clientfd:%d\ncount:%d\nbuffer:%s\n", i, count, buffer);
        }
    }
```

#### 三、使用poll方法

> 其实简单讲讲就是poll和select函数并没有区别，只是两者的接口不一样。

```c
#else
    struct pollfd fds[1024] = {0};

    fds[sockfd].fd = sockfd;
    fds[sockfd].events = POLLIN;
    int maxfd = sockfd;
    while (1)
    {
        int nready = poll(fds, maxfd + 1, -1);
        if (fds[sockfd].revents & POLLIN)
        {
            struct sockaddr_in clientaddr;
            socklen_t len = sizeof(clientaddr);
            int clientfd = accept(sockfd, (struct sockaddr *)&clientaddr, &len);

            printf("sockfd: %d\n", clientfd);
            fds[clientfd].fd = clientfd;
            fds[clientfd].events = POLLIN;

            maxfd = clientfd;
        }
        int i = 0;
        for (i = sockfd + 1; i <= maxfd; ++i)
        {
            if (fds[i].revents & POLLIN)
            {
                char buffer[128] = {0};
                int count = recv(i, buffer, 128, 0);
                if (count == 0)
                {
                    printf("disconnect\n");
                    // !不关闭事件会一直循环循环
                    fds[i].fd = -1;
                    fds[i].events = 0;
                    close(i);

                    continue;
                }
                send(i, buffer, count, 0);
                printf("clientfd:%d\ncount:%d\nbuffer:%s\n", i, count, buffer);
            }
        }
    }

```

#### 四、使用epoll方法

* 首先时epll的组成如下：

  * epoll_create();
  * epoll_ctl();
  * epoll_wait();
* 第二点就是epoll并没有使用到mmap技术！！！

代码实现：

```c
# else
    // create只要参数不为0和负即可
    int epfd = epoll_create(1);
    struct epoll_event ev;
    ev.events = EPOLLIN;
    ev.data.fd = sockfd;
    epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);

    struct epoll_event events[1024] = {0};
    while(1){
        int nready = epoll_wait(epfd,events, 102·4,-1);
        int i = 0;
        for(i = 0;i <nready;++i){
            int connfd = events[i].data.fd;
            if(sockfd == connfd){
                struct sockaddr_in clientaddr;
                socklen_t len = sizeof(clientaddr);
            
                int clientfd = accept(sockfd, (struct sockaddr *)&clientaddr, &len);
            
                ev.events = EPOLLIN;
                ev.data.fd = clientfd;
                epoll_ctl(epfd, EPOLL_CTL_ADD, clientfd, &ev);
            
                printf("sockfd: %d\n", clientfd);
            }
            else if(events[i].events & EPOLLIN)
            {
                char buffer[128] = {0};
                int count = recv(connfd, buffer, 128, 0);
                if (count == 0)
                {
                    printf("disconnect\n");
                    // !不关闭事件会一直循环循环
                
                    epoll_ctl(epfd,EPOLL_CTL_DEL,connfd,NULL);
                    close(i);

                    continue;
                }
                send(connfd, buffer, count, 0);
                printf("clientfd:%d\ncount:%d\nbuffer:%s\n", i, count, buffer);
            }
        }
    }

#endif
```

下面是一些关键函数的注解：

1. `epoll_create`

```c
int epfd = epoll_create(1);
```

* **作用** ：创建epoll实例
* **参数** ：
* `size`：历史遗留参数，现代内核忽略该值（>0即可）
* **返回值** ：epoll文件描述符

1. epoll_ctl

```c
epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);
```

* **作用** ：管理epoll监控列表
* **参数** ：
* `op`：操作类型（ADD/MOD/DEL）
* `fd`：待监控的文件描述符
* `event`：关联的事件结构体
* **事件类型** ：
* `EPOLLIN`：数据可读
* `EPOLLOUT`：数据可写
* `EPOLLET`：边缘触发模式

3. `epoll_wait`

```c
int nready = epoll_wait(epfd, events, 1024, -1);
```

* **作用** ：等待事件发生
* **参数** ：
* `events`：输出参数，存储就绪事件
* `maxevents`：最大接收事件数（需<=数组长度）
* `timeout`：-1表示阻塞等待，0立即返回，>0超时时间(ms)



==**接下来是个重点！！！**==

**水平触发(LT)**以及**边沿触发(ET)**的区别：

- 水平触发LT：只要IO之中还有数据，他就会一直接收，一直触发，然后分条直到全部输出为止

  - > 输入：01234567890123456789
    >
    > 输出：count: 15
    >
    > ​	    buffer: 012345678901234
    >
    > ​	    count: 5
    >
    > ​	    buffer: 56789

- 边沿触发ET：即使IO中还有数据，他也只会读一次，剩下的数据存着，直到下一次有数据输入进来（==理论上可能越积越多==）

  - > 输入：01234567890123456789
    >
    > 输出：count: 15
    >
    > ​	    buffer: 012345678901234
    >
    > 输入：111012345
    >
    > 输出：count: 14
    >
    > ​	    buffer: 56789111012345

- 具体应用场景来说：

  - **LT触发的话**：在tcp的情况下可以设置接受的长度；甚至可以==分包==——截成不同的段落。

  ![image-20250306002224480](进阶学习/image-20250306002224480.png)

  -  **ET触发的用处**：

  ![image-20250306002811957](进阶学习/image-20250306002811957.png)

  - 如果要将一个大小为2g的文件一口度进来，就这样用while  全都进来。

    

#### 五、思考

##### 1. **`epoll` 是否使用 `mmap`？**

**答案：是的，`epoll` 内部使用 `mmap` 优化性能。**

具体机制：

- **内核与用户空间的数据传递**：
  `epoll` 通过 `mmap` 在内核和用户空间之间**共享内存**，避免数据从内核态到用户态的拷贝（零拷贝技术）。当调用 `epoll_wait` 时，事件数据直接通过 `mmap` 映射的内存区域返回，无需传统 `read`/`write` 的多次拷贝。
- **性能优势**：
  这种设计使得 `epoll` 在处理大量文件描述符时效率远高于 `select`/`poll`，尤其是高并发场景（如百万级连接）。

验证方法：

- 通过

  ```
  strace
  ```

  跟踪系统调用可以发现，

  ```
  epoll_create
  ```

  时会调用

  ```
  mmap
  ```

  ```bash
  strace -e trace=mmap,epoll_create your_program
  ```

------



##### 2. **`epoll` 是否线程安全？**

**答案：`epoll` 本身是线程安全的，但需要合理使用。**

详细说明：

- **线程安全的操作**：

  - **`epoll_wait`**：
    多个线程可以同时调用 `epoll_wait` 监听同一个 `epoll` 实例，内核会保证事件分发的原子性。
  - **`epoll_ctl`**：
    对同一个 `epoll` 实例的修改操作（如 `EPOLL_CTL_ADD`/`EPOLL_CTL_MOD`/`EPOLL_CTL_DEL`）需要**加锁保护**，否则可能引发竞争条件。

- **非线程安全的场景**：
  如果多个线程同时对同一个文件描述符调用 `epoll_ctl`，可能导致不可预知的行为。例如：

  ```c
  // 线程1：添加事件
  epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);
  
  // 线程2：删除事件
  epoll_ctl(epfd, EPOLL_CTL_DEL, fd, NULL);
  ```

  若不加锁，这两个操作可能在内核中交错执行，导致状态不一致。

正确用法：

- 使用互斥锁保护 `epoll_ctl`：

  ```c
  pthread_mutex_lock(&epoll_mutex);
  epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);
  pthread_mutex_unlock(&epoll_mutex);
  ```

- **每个线程使用独立的 `epoll` 实例**：
  如果不需要共享事件监听，可以为每个线程创建独立的 `epoll` 实例，避免竞争。

------

代码示例：线程安全的 `epoll` 使用

```c
#include <pthread.h>
#include <sys/epoll.h>

#define MAX_EVENTS 10

int epfd;
pthread_mutex_t epoll_mutex = PTHREAD_MUTEX_INITIALIZER;

void* thread_func(void* arg) {
    struct epoll_event events[MAX_EVENTS];
    while (1) {
        // 多个线程可以安全调用 epoll_wait
        int nready = epoll_wait(epfd, events, MAX_EVENTS, -1);
        for (int i = 0; i < nready; i++) {
            // 处理事件
        }
    }
}

void add_fd_to_epoll(int fd) {
    struct epoll_event ev;
    ev.events = EPOLLIN;
    ev.data.fd = fd;
    
    // 加锁保护 epoll_ctl
    pthread_mutex_lock(&epoll_mutex);
    epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);
    pthread_mutex_unlock(&epoll_mutex);
}
```

------

##### 3. **补充：ET 模式下的线程安全问题**

- **边沿触发（ET）模式**：
  在 ET 模式下，必须确保一次性读取所有数据（循环调用 `recv` 直到返回 `EAGAIN`）。如果多个线程处理同一个 socket 的 `EPOLLIN` 事件，可能导致数据被多个线程重复处理。此时需要设计合理的任务分发机制（如 Leader-Follower 模式）。
- **非阻塞 IO 的必要性**：
  ET 模式下必须将 socket 设为非阻塞模式，否则 `recv` 可能阻塞整个线程，影响其他事件处理。

------

4. `epoll`实现方式划分

![image-20250306010104190](进阶学习/image-20250306010104190.png)

分析：

- 左边明显是**面向IO**的写法
  - 会越来越累赘，越来越复杂
- 右边是**面向事件**的写法
  - 这样的写法本质上就是==reactor==
  - 核心的作用相较于面向IO的写法







### 三、实现reactor方法

本质上是要实现以一个包含这几个东西的连接池，待会还是要细学：

![image-20250306012348490](进阶学习/image-20250306012348490.png)

#### 1. 无终止符，数据一直存储

如果没有收到终止符，读到的每句话都会一直存在buffer中，每次打印全都打印出来。

> clientfd:5
> count:4
> buffer:abcd
>
> clientfd:5
> count:4
> buffer:abcdabcd
>
> clientfd:5
> count:4
> buffer:abcdabcdabcd

这显然是有问题的，但是也有应用场景

我们在浏览器里连接服务器

> http://192.168.1.122:2048  输入地址

会在输出框打印：

> sockfd: 5
> sockfd: 6
> clientfd:5
> count:480
> buffer:GET / HTTP/1.1
> Host: 192.168.1.122:2048
> Connection: keep-alive
> Upgrade-Insecure-Requests: 1
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/133.0.0.0 Safari/537.36 Edg/133.0.0.0
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
> Accept-Encoding: gzip, deflate
> Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6
>
> disconnect



#### 2. reactor的封装

```c
#include <sys/socket.h> // 套接套接字操作相关函数和数据结构
#include <errno.h>      // 错误码定义（如errno）
#include <netinet/in.h> // IPv4地址结构sockaddr_in定义

#include <stdio.h>
#include <string.h>
#include <unistd.h>

#include <sys/epoll.h>
#include <stdlib.h> // 添加此行，声明 malloc 和 free

#define BUFFER_LENGTH 1024

struct conn_item
{
    int fd;
    char buffer[BUFFER_LENGTH];
    int idx;
};

struct conn_item connlist[1024] = {0};

// tcp
int main()
{
    // AF_INET：IPv4地址族。
    // SOCK_STREAM：面向连接的TCP协议。
    // 0：自动选择协议（对于TCP，实际是IPPROTO_TCP）。
    // sockfd：套接字描述符，失败时返回-1。
    int sockfd = socket(AF_INET, SOCK_STREAM, 0);

    struct sockaddr_in serveraddr;

    // 将serveraddr结构体的所有字节置为0，避免残留数据干扰后续操作。
    // 网络编程中结构体可能存在填充字节，未清零可能导致bind()失败
    memset(&serveraddr, 0, sizeof(struct sockaddr_in));

    serveraddr.sin_family = AF_INET;
    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);
    serveraddr.sin_port = htons(2048);

    // bind()_绑定套接字与地址。将套接字关联到特定的IP地址和端口，使客户端能通过该地址访问服务。
    if (-1 == bind(sockfd, (struct sockaddr *)&serveraddr, sizeof(struct sockaddr)))
    {
        perror("bind");
        return -1;
    }

    // 将套接字设置为被动模式，准备接受客户端的连接请求。
    listen(sockfd, 10);

    // create只要参数不为0和负即可
    int epfd = epoll_create(1);
    struct epoll_event ev;
    ev.events = EPOLLIN;
    ev.data.fd = sockfd;
    epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);

    struct epoll_event events[1024] = {0};
    while (1)
    {
        int nready = epoll_wait(epfd, events, 1024, -1);
        int i = 0;
        for (i = 0; i < nready; ++i)
        {
            int connfd = events[i].data.fd;
            if (sockfd == connfd)
            {
                struct sockaddr_in clientaddr;
                socklen_t len = sizeof(clientaddr);

                int clientfd = accept(sockfd, (struct sockaddr *)&clientaddr, &len);

                ev.events = EPOLLIN; // * 水平触发状态，数据没读完就一直触发触发触发！！！
                // * ev.events = EPOLLIN | EPOLLET;  // * 边沿触发状态，数据只触发一次！
                ev.data.fd = clientfd;
                epoll_ctl(epfd, EPOLL_CTL_ADD, clientfd, &ev);

                // ! 初始化部分
                connlist[clientfd].fd = clientfd;
                memset(connlist[clientfd].buffer, 0, BUFFER_LENGTH);
                connlist[clientfd].idx = 0;

                printf("sockfd: %d\n", clientfd);
            }
            else if (events[i].events & EPOLLIN)
            {
                // ! 实现拼接
                char *buffer = connlist[connfd].buffer;
                int idx = connlist[connfd].idx;
                int count = recv(connfd, buffer + idx, BUFFER_LENGTH - idx, 0);

                if (count == 0)
                {
                    printf("disconnect\n");
                    // !不关闭事件会一直循环循环

                    epoll_ctl(epfd, EPOLL_CTL_DEL, connfd, NULL);
                    close(connfd);

                    continue;
                }
                connlist[connfd].idx += count;

                send(connfd, buffer, count, 0);
                printf("clientfd:%d\ncount:%d\nbuffer:%s\n", connfd, count, buffer);
            }
        }
    }

    getchar();
    close(sockfd);
}

```

**首先分析一下这个代码的不足：**

1. 这个代码虽然是根据事件来设计的了，但本质上还是基于io的。
2. 需要修改——有三个事件（回调函数）
   - accept_cb() ==> 针对listenfd
   - recv_cb()  & senf_cb()==> 针对clinentfd



>**EPOLLOUT状态的意义**
>
>==> 判断当前io是否可写
>
>一次出发不关闭就会一直可写（一直发送）
>
>**主要用途的话**
>
>我们监测我们的发送缓存区是否可写，例如我们发送一个文件的时候，可能会超过我们的发送缓存区





上述步骤实现代码：

```c
#include <sys/socket.h> // 套接套接字操作相关函数和数据结构
#include <errno.h>      // 错误码定义（如errno）
#include <netinet/in.h> // IPv4地址结构sockaddr_in定义

#include <stdio.h>
#include <string.h>
#include <unistd.h>

#include <pthread.h>
#include <sys/select.h>
#include <sys/poll.h>
#include <sys/epoll.h>
#include <stdlib.h> // 添加此行，声明 malloc 和 free

#define BUFFER_LENGTH 1024

int epfd = 0;

// * listenfd
// * listenfd 触发 EPOLLIN(事件)， 执行accept_cb()
int accept_cb(int fd);

// * clientfd
// * clientfd 触发 EPOLLIN(事件)， 执行recv_cb()
// * clientfd 触发 EPOLLIN(事件)， 执行send_cb()
int recv_cb(int fd);
int send_cb(int fd);

struct conn_item
{
    int fd;
    char buffer[BUFFER_LENGTH];
    int idx;
};

struct conn_item connlist[1024] = {0};

int accept_cb(int fd)
{
    struct sockaddr_in clientaddr;
    socklen_t len = sizeof(clientaddr);
    struct epoll_event ev;

    int clientfd = accept(fd, (struct sockaddr *)&clientaddr, &len);

    if (clientfd < 0)
        return clientfd;

    ev.events = EPOLLIN; // * 水平触发状态，数据没读完就一直触发触发触发！！！
    // * ev.events = EPOLLIN | EPOLLET;  // * 边沿触发状态，数据只触发一次！
    ev.data.fd = clientfd;
    epoll_ctl(epfd, EPOLL_CTL_ADD, clientfd, &ev);

    // ! 初始化部分
    connlist[clientfd].fd = clientfd;
    memset(connlist[clientfd].buffer, 0, BUFFER_LENGTH);
    connlist[clientfd].idx = 0;

    return clientfd;
}

int recv_cb(int fd)
{
    char *buffer = connlist[fd].buffer;
    int idx = connlist[fd].idx;
    int count = recv(fd, buffer + idx, BUFFER_LENGTH - idx, 0);

    if (count == 0)
    {
        printf("disconnect\n");
        //* 怎么解决缺少epollfd的问题
        //* 1.定义为全局变量
        //* 2.定义一个reactor结构体
        epoll_ctl(epfd, EPOLL_CTL_DEL, fd, NULL);
        close(fd);
    }
    connlist[fd].idx += count;

    // 设置事件
    struct epoll_event ev;
    ev.events = EPOLLOUT;
    ev.data.fd = fd;
    epoll_ctl(epfd, EPOLL_CTL_MOD, fd, &ev);

    // send(fd, buffer, count, 0);
    return count;
}

int send_cb(int fd)
{
    char *buffer = connlist[fd].buffer;
    int idx = connlist[fd].idx;

    int count = send(fd, buffer, idx, 0);

    struct epoll_event ev;
    ev.events = EPOLLIN;
    ev.data.fd = fd;
    epoll_ctl(epfd, EPOLL_CTL_MOD, fd, &ev);

    return count;
}

// tcp
int main()
{
    int sockfd = socket(AF_INET, SOCK_STREAM, 0);

    struct sockaddr_in serveraddr;

    memset(&serveraddr, 0, sizeof(struct sockaddr_in));

    serveraddr.sin_family = AF_INET;
    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);
    serveraddr.sin_port = htons(2048);

    // bind()_绑定套接字与地址。将套接字关联到特定的IP地址和端口，使客户端能通过该地址访问服务。
    if (-1 == bind(sockfd, (struct sockaddr *)&serveraddr, sizeof(struct sockaddr)))
    {
        perror("bind");
        return -1;
    }

    // 将套接字设置为被动模式，准备接受客户端的连接请求。
    listen(sockfd, 10);

    // create只要参数不为0和负即可
    epfd = epoll_create(1);
    struct epoll_event ev;
    ev.events = EPOLLIN;
    ev.data.fd = sockfd;
    epoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);

    struct epoll_event events[1024] = {0};
    while (1)
    {
        int nready = epoll_wait(epfd, events, 1024, -1);
        int i = 0;
        for (i = 0; i < nready; ++i)
        {
            int connfd = events[i].data.fd;
            if (sockfd == connfd)
            {
                int clientfd = accept_cb(sockfd);
                printf("sockfd: %d\n", clientfd);
            }
            else if (events[i].events & EPOLLIN)
            {
                int count = recv_cb(connfd);
                printf("recv <-- buffer:%s\n", connlist[connfd].buffer);
            }
            else if (events[i].events & EPOLLOUT)
            {
                int count = send_cb(connfd);
                printf("send --> buffer:%s\n", connlist[connfd].buffer);
            }
        }
    }

    getchar();
    // 不要将关闭交给服务端，否则另一边端口不会释放，会一直报Timeout
    close(sockfd);
}

```





#### 3. 使用回调函数进行解决问题

这边的话，可以使用回调函数进行更加强的事件模式转变，进一步去除IO模式的影响。

具体的实现代码如下：

```
#include <sys/socket.h> 
#include <errno.h>      
#include <netinet/in.h> 

#include <stdio.h>
#include <string.h>
#include <unistd.h>

#include <pthread.h>
#include <sys/epoll.h>
#include <stdlib.h> 

#define BUFFER_LENGTH 1024

// 定义回调函数类型
typedef int (*RCALLBACK)(int fd);

// 前向声明回调函数
int accept_cb(int fd);
int recv_cb(int fd);
int send_cb(int fd);

/**
 * 连接项结构体，保存每个连接的信息
 * fd: 文件描述符
 * buffer: 数据缓冲区
 * idx: 当前缓冲区中的数据长度
 * recv_t: 接收回调联合体（监听socket用accept，普通socket用recv）
 * send_callback: 发送回调函数
 * @bug accept_callback: Output extra lines
 * TODO: Fix the above bugs
 */
struct conn_item
{
    int fd;
    char buffer[BUFFER_LENGTH];
    int idx;
    union {
        RCALLBACK accept_callback; // 用于监听socket的accept回调
        RCALLBACK recv_callback;   // 用于普通socket的recv回调
    } recv_t;
    RCALLBACK send_callback;       // 发送回调
};

int epfd = 0; // epoll实例的文件描述符
struct conn_item connlist[1024] = {0}; // 连接项数组，索引为文件描述符

/**
 * 设置epoll事件
 * @param fd 需要设置的文件描述符
 * @param event 事件类型（EPOLLIN/EPOLLOUT）
 * @param flag 0表示修改现有事件，非0表示添加新事件
 * @return 始终返回0
 */
int set_event(int fd, int event, int flag)
{
    struct epoll_event ev;
    ev.events = event;
    ev.data.fd = fd;

    if (flag) {
        // 添加新事件
        epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);
    } else {
        // 修改现有事件
        epoll_ctl(epfd, EPOLL_CTL_MOD, fd, &ev);
    }
    return 0;
}

/**
 * 接受连接的回调函数
 * @param fd 监听socket的文件描述符
 * @return 成功返回clientfd，失败返回-1
 */
int accept_cb(int fd)
{
    struct sockaddr_in clientaddr;
    socklen_t len = sizeof(clientaddr);
    // 接受新连接
    int clientfd = accept(fd, (struct sockaddr*)&clientaddr, &len);
    if (clientfd < 0) return clientfd;

    // 将新连接的socket加入epoll监听读事件
    set_event(clientfd, EPOLLIN, 1); // 第三个参数1表示添加新事件

    // 初始化连接信息
    connlist[clientfd].fd = clientfd;
    memset(connlist[clientfd].buffer, 0, BUFFER_LENGTH);
    connlist[clientfd].idx = 0;
    printf("client: %d\n", connlist[clientfd].fd);
    // 设置回调函数
    connlist[clientfd].recv_t.recv_callback = recv_cb;
    connlist[clientfd].send_callback = send_cb;

    return clientfd;
}

/**
 * 接收数据的回调函数
 * @param fd 数据来源的socket文件描述符
 * @return 接收到的字节数，连接关闭返回0
 */
int recv_cb(int fd)
{
    char* buffer = connlist[fd].buffer;
    int idx = connlist[fd].idx;
    // 接收数据
    int count = recv(fd, buffer + idx, BUFFER_LENGTH - idx, 0);

    if (count == 0) { // 连接关闭
        printf("disconnect\n");
        epoll_ctl(epfd, EPOLL_CTL_DEL, fd, NULL);
        close(fd);
    } else if (count > 0) {
        connlist[fd].idx += count; // 更新缓冲区索引
    }
    
    // 修改事件为监听写事件（准备发送数据）
    set_event(fd, EPOLLOUT, 0); // 第三个参数0表示修改事件
    return count;
}

/**
 * 发送数据的回调函数
 * @param fd 目标socket文件描述符
 * @return 发送的字节数
 */
int send_cb(int fd)
{
    char* buffer = connlist[fd].buffer;
    int idx = connlist[fd].idx;
    // 发送数据
    int count = send(fd, buffer, idx, 0);
    // 修改事件为监听读事件（准备接收下一批数据）
    set_event(fd, EPOLLIN, 0);
    return count;
}

// 主函数（TCP服务器）
int main()
{
    // 创建监听socket
    int sockfd = socket(AF_INET, SOCK_STREAM, 0);
    struct sockaddr_in serveraddr;
    memset(&serveraddr, 0, sizeof(serveraddr));
    serveraddr.sin_family = AF_INET;
    serveraddr.sin_addr.s_addr = htonl(INADDR_ANY);
    serveraddr.sin_port = htons(2048);

    // 绑定地址
    if (bind(sockfd, (struct sockaddr*)&serveraddr, sizeof(serveraddr)) == -1) {
        perror("bind");
        return -1;
    }

    // 开始监听
    listen(sockfd, 10);

    // 初始化监听socket的连接项
    connlist[sockfd].fd = sockfd;
    connlist[sockfd].recv_t.accept_callback = accept_cb; // 设置accept回调

    // 创建epoll实例
    epfd = epoll_create(1);
    // 将监听socket加入epoll
    set_event(sockfd, EPOLLIN, 1); // 第三个参数1表示添加新事件

    struct epoll_event events[1024] = {0};
    while (1) {
        // 等待事件发生
        int nready = epoll_wait(epfd, events, 1024, -1);
        for (int i = 0; i < nready; ++i) {
            int connfd = events[i].data.fd;
            // 处理读事件（包括新连接和数据到达）
            if (events[i].events & EPOLLIN) {
                // 调用对应的回调函数
                // ! bug is created by this lines, the recv_t is accept, but the printf function is still running.
                int count = connlist[connfd].recv_t.recv_callback(connfd);
                printf("recv[count: %d] <-- buffer:%s\n", count, connlist[connfd].buffer);
            } 
            // 处理写事件（数据可发送）
            else if (events[i].events & EPOLLOUT) {
                int count = connlist[connfd].send_callback(connfd);
                printf("send[count: %d] --> buffer:%s\n", count, connlist[connfd].buffer);
            }
        }
    }

    close(sockfd);
}
```

这里的话，主要是有个小bug，在我们的状态是accept的时候，我们不应该输出这个句子：

> if (events[i].events & EPOLLIN) {
>
> ​        // 调用对应的回调函数
>
> ​        // ! bug is created by this lines, the recv_t is accept, but the printf function is still running.
>
> ​        int count = connlist[connfd].recv_t.recv_callback(connfd);
>
> ​        printf("recv[count: %d] <-- buffer:%s\n", count, connlist[connfd].buffer);
>
> ​      } 

==这边只能说不输出就是最好的选择==



除此以外，tips：

> `union`是什么呢？
>
> 简单来说就是一个将中间包含的同类的变量绑定到一个同样的内存空间去，你设置union变量是什么，他就会在这个内存空间中将union变量变成什么！



#### ==思考题==

1.  能不能把connfd从主循环中剔除，使得主循环只关注我们的IN和OUT参数。
2. select、poll、epoll将他们封装在一个完整的代码中。







#### 4. connlist[1024]进行数量上的扩展

首先，我们需要明确，reactor是干了一个什么事情：

![image-20250310002605093](进阶学习/image-20250310002605093.png)

如这张图所示，我们进行reactor的设计之后的话，我们的一个io本质上，我们具体怎么通信，怎么实现都已经不重要了。

我们现在实际上==只需要关注两个点：==`rbuffer` `wbuffer`，当这两个里面有数据的时候就执行，读数据和写数据的操作。如果没数据的话就保持连接这种，接收到为0则终止

了不得我们在添加一个set_event()来进行一个状态的设计！！！



所以现在我们需要隔离开来rbuffer和wbuffer

具体实现代码如下：

```c
#define BUFFER_LENGTH 1024

// 定义回调函数类型
typedef int (*RCALLBACK)(int fd);

// 前向声明回调函数
int accept_cb(int fd);
int recv_cb(int fd);
int send_cb(int fd);

/**
 * 连接项结构体，保存每个连接的信息
 * @fd: 文件描述符
 * @rbuffer: 数据接收缓冲区
 * @rlen: 接收缓冲区中的数据长度
 * @wbuffer: 数据发送缓冲区
 * @wlen: 发送缓冲区中的数据长度
 * @recv_t: 接收回调联合体（监听socket用accept，普通socket用recv）
 * @send_callback: 发送回调函数
 * @bug accept_callback: Output extra lines
 * TODO: Fix the above bugs
 */
struct conn_item
{
    int fd;

    char rbuffer[BUFFER_LENGTH];
    int rlen;

    char wbuffer[BUFFER_LENGTH];
    int wlen;

    union
    {
        RCALLBACK accept_callback; // 用于监听socket的accept回调
        RCALLBACK recv_callback;   // 用于普通socket的recv回调
    } recv_t;
    RCALLBACK send_callback; // 发送回调
};

int epfd = 0;                          // epoll实例的文件描述符
struct conn_item connlist[1024] = {0}; // 连接项数组，索引为文件描述符

/**
 * 设置epoll事件
 * @param fd 需要设置的文件描述符
 * @param event 事件类型（EPOLLIN/EPOLLOUT）
 * @param flag 0表示修改现有事件，非0表示添加新事件
 * @return 始终返回0
 */
int set_event(int fd, int event, int flag)
{
    struct epoll_event ev;
    ev.events = event;
    ev.data.fd = fd;

    if (flag)
    {
        // 添加新事件
        epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);
    }
    else
    {
        // 修改现有事件
        epoll_ctl(epfd, EPOLL_CTL_MOD, fd, &ev);
    }
    return 0;
}

/**
 * 接受连接的回调函数
 * @param fd 监听socket的文件描述符
 * @return 成功返回clientfd，失败返回-1
 */
int accept_cb(int fd)
{
    struct sockaddr_in clientaddr;
    socklen_t len = sizeof(clientaddr);
    // 接受新连接
    int clientfd = accept(fd, (struct sockaddr *)&clientaddr, &len);
    if (clientfd < 0)
        return clientfd;

    // 将新连接的socket加入epoll监听读事件
    set_event(clientfd, EPOLLIN, 1); // 第三个参数1表示添加新事件

    // 初始化连接信息
    connlist[clientfd].fd = clientfd;
    memset(connlist[clientfd].rbuffer, 0, BUFFER_LENGTH);
    connlist[clientfd].rlen = 0;
    memset(connlist[clientfd].wbuffer, 0, BUFFER_LENGTH);
    connlist[clientfd].wlen = 0;
    printf("client: %d\n", connlist[clientfd].fd);
    // 设置回调函数
    connlist[clientfd].recv_t.recv_callback = recv_cb;
    connlist[clientfd].send_callback = send_cb;

    return clientfd;
}

/**
 * 接收数据的回调函数
 * @param fd 数据来源的socket文件描述符
 * @return 接收到的字节数，连接关闭返回0
 */
int recv_cb(int fd)
{
    char *buffer = connlist[fd].rbuffer;
    int idx = connlist[fd].rlen;
    // 接收数据
    int count = recv(fd, buffer + idx, BUFFER_LENGTH - idx, 0);

    if (count == 0)
    { // 连接关r闭
        printf("disconnect\n");
        epoll_ctl(epfd, EPOLL_CTL_DEL, fd, NULL);
        close(fd);
        return -1;
    }
    else if (count > 0)
    {
        connlist[fd].rlen += count; // 更新缓冲区索引
    }


```



这边其实有个重点，就是将我们的rbuffer复制到wbuffer：

```c
#if 1 // echo: need to send
    memcpy(connlist[fd].wbuffer, connlist[fd].rbuffer, connlist[fd].rlen);
    connlist[fd].wlen = connlist[fd].rlen;
#endif
```

这个很重要！！！

## http server的搭建

- 字符
- html
- png

### 一、具体实现

首先我们HTTP连接的时候会接收到一串如下==请求头==：

> GET / HTTP/1.1
> Host: 192.168.1.122:2048
> Connection: keep-alive
> Cache-Control: max-age=0
> Upgrade-Insecure-Requests: 1
> User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36 Edg/134.0.0.0
> Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
> Accept-Encoding: gzip, deflate
> Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6



我们的回复暂定设置为：

> "HTTP/1.1 200 OK\r\n"
>      "Accept-Ranges: bytes\r\n"
>      "Content-Length: 82\r\n"
>      "Content-Type: text/html\r\n"
>      "Date: Sat, 06 Aug 2023 13:16:46 GMT\r\n\r\n"
>      "<html><head><title>0voice.king</title></head><body><h1>King</h1></body></html>\r\n\r\n");





通过这样设置：

```c
#if ENABLE_HTTP_RESPONSE
typedef struct conn_item connection_t;

int http_response(connection_t *conn){
    conn->wlen = sprintf(conn->wbuffer, 
		"HTTP/1.1 200 OK\r\n"
		"Accept-Ranges: bytes\r\n"
		"Content-Length: 82\r\n"
		"Content-Type: text/html\r\n"
		"Date: Sat, 06 Aug 2023 13:16:46 GMT\r\n\r\n"
		"<html><head><title>0voice.king</title></head><body><h1>King</h1></body></html>\r\n\r\n");
}
#endif
.........................................
int recv_cb(int fd)
{
    char *buffer = connlist[fd].rbuffer;
    int idx = connlist[fd].rlen;
    // 接收数据
    int count = recv(fd, buffer + idx, BUFFER_LENGTH - idx, 0);

    if (count == 0)
    { // 连接关r闭
        printf("disconnect\n");
        epoll_ctl(epfd, EPOLL_CTL_DEL, fd, NULL);
        close(fd);
        return -1;
    }
    else if (count > 0)
    {
        connlist[fd].rlen += count; // 更新缓冲区索引
    }

// 解决发送字符串问题
#if 0 // echo: need to send
    memcpy(connlist[fd].wbuffer, connlist[fd].rbuffer, connlist[fd].rlen);
    connlist[fd].wlen = connlist[fd].rlen;

#else
    // http_request(&connlist[fd]);
    http_response(&connlist[fd]);
#endif

    // 修改事件为监听写事件（准备发送数据）
    set_event(fd, EPOLLOUT, 0); // 第三个参数0表示修改事件
    return count;
}

```



通过上述方式可以在网页中输出一个king。



同样我们可以将我们需要输出的网页保存为html,然后输出：

```c
int http_response(connection_t *conn)
{
#if 0
    conn->wlen = sprintf(conn->wbuffer,
        "HTTP/1.1 200 OK\r\n"
        "Accept-Ranges: bytes\r\n"
        "Content-Length: 82\r\n"
        "Content-Type: text/html\r\n"
        "Date: Sat, 06 Aug 2023 13:16:46 GMT\r\n\r\n"
        "<html><head><title>0voice.king</title></head><body><h1>King</h1></body></html>\r\n\r\n");

#else
    int filefd = open("index.html", O_RDONLY);

    struct stat stat_buf;
    fstat(filefd, &stat_buf);

    conn->wlen = sprintf(conn->wbuffer,
        "HTTP/1.1 200 OK\r\n"
        "Accept-Ranges: bytes\r\n"
        "Content-Length: %ld\r\n"
        "Content-Type: text/html\r\n"
        "Date: Sat, 06 Aug 2023 13:16:46 GMT\r\n\r\n",
        stat_buf.st_size);
    int count = read(filefd, conn->wbuffer+conn->wlen,BUFFER_LENGTH-conn->wlen);
    conn->wlen += count;

#endif
    return conn->wlen;
}

#endif
```





### 二、如何测试http服务器性能

==工具：wrk==

地址：

> htttps://github.com/wg/wrk.git











# 25年3月12日

## 2.2.1 服务器的百万并发实现

![image-20250312001003123](进阶学习/image-20250312001003123.png)

今天的重点！！！**网络部分并发！！！**

### 一、实现代码

首先的话，我们的基础框架还是之前课上讲到的reactor服务器框架，之后我们使用一个测试软件在另一台服务器客户端上对他进行测试。

测试代码如下：（默认最大测试340000）为满足百万并发测试

```c


#include <stdio.h>
#include <string.h>
#include <stdlib.h>

#include <sys/types.h>
#include <sys/socket.h>
#include <sys/epoll.h>
#include <sys/time.h>
#include <errno.h>
#include <netinet/tcp.h>
#include <arpa/inet.h>
#include <netdb.h>
#include <fcntl.h>


#define MAX_BUFFER		128
#define MAX_EPOLLSIZE	(384*1024)
#define MAX_PORT		20

#define TIME_SUB_MS(tv1, tv2)  ((tv1.tv_sec - tv2.tv_sec) * 1000 + (tv1.tv_usec - tv2.tv_usec) / 1000)

int isContinue = 0;

static int ntySetNonblock(int fd) {
	int flags;

	flags = fcntl(fd, F_GETFL, 0);
	if (flags < 0) return flags;
	flags |= O_NONBLOCK;
	if (fcntl(fd, F_SETFL, flags) < 0) return -1;
	return 0;
}

static int ntySetReUseAddr(int fd) {
	int reuse = 1;
	return setsockopt(fd, SOL_SOCKET, SO_REUSEADDR, (char *)&reuse, sizeof(reuse));
}



int main(int argc, char **argv) {
	if (argc <= 2) {
		printf("Usage: %s ip port\n", argv[0]);
		exit(0);
	}

	const char *ip = argv[1];
	int port = atoi(argv[2]);
	int connections = 0;
	char buffer[128] = {0};
	int i = 0, index = 0;

	struct epoll_event events[MAX_EPOLLSIZE];
	
	int epoll_fd = epoll_create(MAX_EPOLLSIZE);
	
	strcpy(buffer, " Data From MulClient\n");
		
	struct sockaddr_in addr;
	memset(&addr, 0, sizeof(struct sockaddr_in));
	
	addr.sin_family = AF_INET;
	addr.sin_addr.s_addr = inet_addr(ip);

	struct timeval tv_begin;
	gettimeofday(&tv_begin, NULL);

	while (1) {
		if (++index >= MAX_PORT) index = 0;
		
		struct epoll_event ev;
		int sockfd = 0;

		if (connections < 340000 && !isContinue) {
			sockfd = socket(AF_INET, SOCK_STREAM, 0);
			if (sockfd == -1) {
				perror("socket");
				goto err;
			}

			//ntySetReUseAddr(sockfd);
			addr.sin_port = htons(port);

			if (connect(sockfd, (struct sockaddr*)&addr, sizeof(struct sockaddr_in)) < 0) {
				perror("connect");
				goto err;
			}
			ntySetNonblock(sockfd);
			ntySetReUseAddr(sockfd);

			sprintf(buffer, "Hello Server: client --> %d\n", connections);
			send(sockfd, buffer, strlen(buffer), 0);

			ev.data.fd = sockfd;
			ev.events = EPOLLIN | EPOLLOUT;
			epoll_ctl(epoll_fd, EPOLL_CTL_ADD, sockfd, &ev);
		
			connections ++;
		}
		//connections ++;
		if (connections % 1000 == 999 || connections >= 340000) {
			struct timeval tv_cur;
			memcpy(&tv_cur, &tv_begin, sizeof(struct timeval));
			
			gettimeofday(&tv_begin, NULL);

			int time_used = TIME_SUB_MS(tv_begin, tv_cur);
			printf("connections: %d, sockfd:%d, time_used:%d\n", connections, sockfd, time_used);

			int nfds = epoll_wait(epoll_fd, events, connections, 100);
			for (i = 0;i < nfds;i ++) {
				int clientfd = events[i].data.fd;

				if (events[i].events & EPOLLOUT) {
					sprintf(buffer, "data from %d\n", clientfd);
					send(sockfd, buffer, strlen(buffer), 0);
				} else if (events[i].events & EPOLLIN) {
					char rBuffer[MAX_BUFFER] = {0};				
					ssize_t length = recv(sockfd, rBuffer, MAX_BUFFER, 0);
					if (length > 0) {
						printf(" RecvBuffer:%s\n", rBuffer);

						if (!strcmp(rBuffer, "quit")) {
							isContinue = 0;
						}
						
					} else if (length == 0) {
						printf(" Disconnect clientfd:%d\n", clientfd);
						connections --;
						close(clientfd);
					} else {
						if (errno == EINTR) continue;

						printf(" Error clientfd:%d, errno:%d\n", clientfd, errno);
						close(clientfd);
					}
				} else {
					printf(" clientfd:%d, errno:%d\n", clientfd, errno);
					close(clientfd);
				}
			}
		}

		usleep(500);
	}

	return 0;

err:
	printf("error : %s\n", strerror(errno));
	return 0;
	
}

```



### 二、问题汇总及分析

#### 1. 客户端测试时卡死不动

这个的话需要在命令行窗口中做出修改，主要是我们的 `ulimit -n` 我们需要修改最大接入文件数，这点的话实际上是在几个客户端和服务端都要进行设置的

>open files                          (-n) 1048576

我是设置成了这么大，方便后续调整吧。



#### 2. reactor（服务端）报错connection refused

> core dumped

![image-20250313223916413](进阶学习/image-20250313223916413.png)

这个首先需要解决我们的服务端代码中有的接收上限设置为1024的问题

这个地方的话，将服务端的代码修改一下接收的上线**（我改为了65535），这个后续还要改到百万**



当然这个时候我们跑到结束的话会存在一个问题就是等到头来还是会有connection refused这样的问题，==但只能说我们的修改是有用的。==

![image-20250313230155894](进阶学习/image-20250313230155894.png)





#### 3. 客户端连接  报错cannot assign requested adress

![image-20250313230833011](进阶学习/image-20250313230833011.png)

这边的话要讲到！！！==我们一个电脑的话最多只有65535个端口==，这个时候如果超出就会报错，而且0-1024个端口是被系统限死。所以会超出就不行了。（五元不足！！就是源目标这个五个信息！）



同时如果不到65535就报错比如说三万多的话，可能话有个原因就是**系统设置了可用的端口号范围**，这个时候就需要==手动修改==了！！！

![image-20250313233725406](进阶学习/image-20250313233725406.png)



修改以后如图所示：

![image-20250317232725545](进阶学习/image-20250317232725545.png)

### 三、server监听端口数量增加

简单来说就是用循环的方式，对多个端口进行监听，从2048到2148

#### 1. 代码实现

首先我们就是修改reactor代码，对其中的话，添加循环检测多个端口以及添加一个时间显示的部分内容。

具体修改内容如下：

```c
int accept_cb(int fd)
{
	............
    if ((clientfd % 1000) == 999)
    {
        struct timeval tv_cur;
        gettimeofday(&tv_cur,NULL);
        int time_used  = TIME_SUB_MS(tv_cur, reactor_begin);
        memcpy(&reactor_begin, &tv_cur, sizeof(struct timeval));
        printf("cliendfd: %d, time_used: %d\n", conn->fd, time_used);
    }

    return clientfd;
}

// =============================================

int Init_server(unsigned short port)
{
    // 初始化监听套接字
    int sockfd = socket(AF_INET, SOCK_STREAM, 0);
    struct sockaddr_in serveraddr = {
        .sin_family = AF_INET,
        .sin_addr.s_addr = htonl(INADDR_ANY),
        .sin_port = htons(port)};

    if (bind(sockfd, (struct sockaddr *)&serveraddr, sizeof(serveraddr)) == -1)
    {
        perror("Bind failed");
        return -1;
    }
    listen(sockfd, 100);
    return sockfd;
}

/**
 * @brief 主函数（TCP服务器入口）
 * @return 程序退出状态码
 */
int main()
{
    int port_count = 100;
    unsigned short port = 2048;
    int i = 0;

    // 创建epoll实例
    epfd = epoll_create(1);

    for (i = 0; i < port_count; ++i)
    {
        int sockfd = Init_server(i + port);
        // 初始化监听套接字连接项
        connlist[sockfd].fd = sockfd;
        connlist[sockfd].recv_t.accept_callback = accept_cb;
        set_event(sockfd, EPOLLIN, 1);
    }


```



#### 2. 内存申请bug

不过这个时候就会产生一个很严重的报错问题，实际上还是内存申请的问题

![image-20250317224316697](进阶学习/image-20250317224316697.png)

![image-20250317224301076](进阶学习/image-20250317224301076.png)

如果想要解决这个问题的话，无非是两种，一个是限制这里的可以创建的端口数量，另一个的话是修改前面的我们设置的接收和发送缓冲区的长度！！！

![image-20250317224408879](进阶学习/image-20250317224408879.png)

将长度修改为512，将内存大小需求将为1G。



#### 3. 还是会有卡65535的问题（不一定）

同时  就算前面我们有修改了openfiles的最大值为1048576，以及刚刚的端口范围设置

>open files                          (-n) 1048576

![image-20250317232725545](进阶学习/image-20250317232725545.png)

我们可能还是需要额外设置一下：

![image-20250317233754588](进阶学习/image-20250317233754588.png)





#### 4.退出程序的时候为什么直接结束

就是我们有的时候运行代码，接收到几十万的时候会自动关闭是什么呢![image-20250317235035322](进阶学习/image-20250317235035322.png)

这个时候可能就是因为我们物理层层面需要我们设置一下系统的接收缓冲区内存和发送缓冲区内存，具体修改如下，报错应对如此。

**简单来说就是可能会被系统杀掉**

![image-20250317235600812](进阶学习/image-20250317235600812.png)



### 四、客户端程序kill发生雪崩

![image-20250317235600812](进阶学习/image-20250317235600812.png)

简单来说就是我们的客户端被kill掉的时候，我们的服务端的话，close操作是发生在receive中的，所以当大量的关闭申请发送到服务端的时候，就会导致我们的服务端发生==雪崩现象==，直接崩溃掉



这个时候我们就需要重复上述步骤！！！扩大我们的内存分配！！！！！

这个内存是需要跟内核版本挂钩的。







# 25年3月18日

## 2.2.2 Posix API 与网络协议栈

### 一、posix API

**API的大致种类**

![image-20250318210819136](进阶学习/image-20250318210819136.png)

#### **1. 建立连接**

> ==**socket():**==
>
> 1. 插座，插，座
> 2. 网络io，<fd-tcp control block>
>    - 分配fd, int  (使用bitmap的算法，简单来说1，2，3依次检测，置1被用，置0可用)
>    - tcb, alloc()
>
> ---
>
> - 用法：
>
>   ```
>   int socket(int domain, int type, int protocol);
>   ```
>
>   - `domain`: AF_INET(IPv4)/AF_INET6(IPv6)
>   - `type`: SOCK_STREAM(TCP)/SOCK_DGRAM(UDP)
>
> - 原理：
>
>   - 内核创建套接字结构体（包含发送/接收缓冲区、协议族、状态等）
>   - 返回文件描述符映射到内核套接字对象
>
> - 注意事项：
>
>   - 需检查返回值（-1表示失败）
>   - 创建后默认是阻塞模式



> ==**bind(fd , );**==
>   a. ip, port
>
> 简单来说就是把这里的ip端口地址绑定到前面的tcb上面去（其实就是一个set的过程）
>
> ==所以我们的客户端可以不设置，他会自动选择一个端口==
>
> ---
>
> - 用法：
>
>   ```
>   int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen);
>   ```
>
>   - 服务器必须绑定，客户端通常由系统自动分配端口
>
> - 原理：
>
>   - 将IP+Port与套接字关联
>   - 内核检查端口可用性（避免端口冲突）
>
> - 注意事项：
>
>   - 使用`setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR)`解决`TIME_WAIT`状态端口占用问题



> ==**listen();**==
> a. tcb->status = TCP_STATUS_LISTEN;
> b. tcb->syn_queue
>   tcb->accept_queue
>
> 这个函数简单来说是要将tcb状态设置为listen状态；
>
> 同时在这里分配一个全连接队列和一个半连接队列
>
> ---
>
> - 用法：
>
>   ```
>   int listen(int sockfd, int backlog);
>   ```
>
>   - backlog：已完成连接队列的最大长度（Linux中实际是未完成+已完成队列）
>   - ==**listen(fd, backlog);**==
>     1. **syn队列**
>     2. **syn + accept队列总长度，未分配fd的tcb的数量**
>     3. **accept队列长度**
>
> - 原理：
>
>   - 将主动套接字转为被动套接字
>   - 创建SYN队列（半连接队列）和ACCEPT队列（全连接队列）
>
> - 注意事项：
>
>   - Linux内核2.2后backlog参数含义变化（实际值由`/proc/sys/net/core/somaxconn`决定）

![image-20250318215447797](进阶学习/image-20250318215447797.png)

#### ***三次握手**

![image-20250318215658668](进阶学习/image-20250318215658668.png)

> **具体我们的三次握手是做了什么呢？**
>
> 结合这个tcp包来看，
>
> 1. 当我们解析了以太网、ip头以及tcp头之后的话，如果解析到了我们的syn位被置了1且seqnum这个有值（1234只是举例可以随机）
> 2. 第二次的话就会令ack=1，acknum置为seqnum+1，
>    同时syn置1，seqnum随机
> 3. 第三次就会返回ack = 1，acknum为之前的seqnum+1.
>
> ==不丢失、不重复、不乱序==
>
> ![image-20250318215749910](进阶学习/image-20250318215749910.png)



==**三次握手的函数有那些？**==

1. listen()的backlog本身类似饭店吃饭排队取的号
2. 当我们监听到listen信息的时候就会先产生半连接==syn==
3. 三次握手之后才会使全连接accept()

==**DDOS攻击！！**==

1. 本质上就是靠tcp连接只进行第一步，就是对面服务端linsten到了第一个数据。之后的话，客户端不给他第三次回复
2. 怎么防护：



>部分问题：
>
>**a. tcp连接的生命周期，从什么时候开始？**
>
>​	真实起点：
>
>- **客户端视角**：从调用`connect()`发送SYN包开始（进入`SYN_SENT`状态）
>- **服务端视角**：从收到SYN包并创建半连接条目开始（进入`SYN_RECV`状态）
>
>**生命周期完整流程**
>
>```mermaid
>stateDiagram-v2
>    [*] --> CLOSED
>    CLOSED --> SYN_SENT: connect()
>    SYN_SENT --> ESTABLISHED: 收到SYN+ACK
>    CLOSED --> LISTEN: listen()
>    LISTEN --> SYN_RECV: 收到SYN
>    SYN_RECV --> ESTABLISHED: 收到ACK
>    ESTABLISHED --> FIN_WAIT_1: close()
>    FIN_WAIT_1 --> FIN_WAIT_2: 收到ACK
>    FIN_WAIT_2 --> TIME_WAIT: 收到FIN
>```
>
>##### **关键澄清**
>
>- IP地址绑定发生在`bind()`阶段（早于连接建立）
>- 真正的TCP连接生命周期始于三次握手，终结于四次挥手
>
>
>
>**b. 第三握手数据包，如何从半连接队列查找匹配的节点**
>
>**半连接队列核心机制**
>
>- **数据结构**：内核维护`syn_table`哈希表存储半连接请求
>
>- **条目结构**
>
>  ```c
>  struct inet_request_sock {
>      __be32          ir_rmt_addr;  // 客户端IP
>      __be16          ir_rmt_port;  // 客户端端口
>      __u32           ir_mark;      // 路由标记
>      struct tcp_options_received rx_opt; // 接收的TCP选项
>      u32             snt_isn;      // 服务端初始序列号
>  };
>  ```
>
>##### **ACK包匹配流程**
>
>1. 提取五元组：
>
>   - 源IP、源端口、目的IP、目的端口、协议类型
>
>2. 哈希快速定位：
>
>   ```c
>   hash = inet_ehashfn(saddr, sport, daddr, dport);
>   bucket = hash & (INET_LHTABLE_SIZE - 1);
>   ```
>
>3. 序列号验证：
>
>   - 检查ACK号是否等于`irs->snt_isn + 1`
>
>4. 状态升级：
>
>   - 将条目从`syn_table`迁移到`ehash`（已连接哈希表）
>
>
>
>**c. syn泛洪**
>
>**攻击原理**
>
>```mermaid
>graph LR
>    A[攻击者] -->|发送伪造源IP的SYN包| B(服务端半连接队列)
>    B -->|队列满| C[拒绝新连接]
>```
>
>##### **内核防御机制**
>
>1. **SYN Cookies**
>
>   - **触发条件**：当半连接队列超过`net.ipv4.tcp_max_syn_backlog`
>
>   - 算法流程：
>
>     ```markdown
>     cookie = hash(saddr, daddr, sport, dport) + 序列号计数器
>     将cookie编码到SYN+ACK的初始序列号中
>     ```
>
>   - 验证过程：
>
>     ```c
>     if (ack_seq - 1 == hash(saddr, sport, daddr, dport) + counter)
>         accept connection
>     ```
>
>2. **动态队列调整**
>
>   - 参数控制：
>
>     ```bash
>     sysctl -w net.ipv4.tcp_syncookies=2 # 始终启用
>     sysctl -w net.ipv4.tcp_synack_retries=3 # 减少重试次数
>     ```
>
>**系统监控命令**
>
>```bash
># 查看半连接队列溢出
>netstat -s | grep "SYNs to LISTEN"
># 查看当前SYN_RECV状态连接
>ss -n state syn-recv
>```
>
>------
>
>**进阶思考**
>
>1. SYN Cookies的密码学弱点
>   - 早期使用MD5存在碰撞风险，现代内核采用SHA-256
>2. 硬件加速防御
>   - DPDK实现SYN代理，在用户态过滤恶意包
>3. 云环境特殊处理
>   - AWS的Nitro Cards实现SYN预处理卸载



> ==**accept();**==
>
> 1. 分配fd 
> 2. fd --> tcb 
>
> 回顾一下我们之前学的：
>
> **listenfd**
>    accept();
>
> **clientfd**
>    recv();
>    send();
>
> ---
>
> **用法**：`int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);`
>
> **原理**：
>
> - 从ACCEPT队列取出已建立的连接
> - 创建新的文件描述符指向新连接
>
> **注意事项**：
>
> - 返回的新套接字与原监听套接字无关联
> - 必须处理EAGAIN/EWOULDBLOCK错误（非阻塞模式下）



**思考题：如果使用ET（边沿触发）模式的话，如何接收所有信息**

```c
while(1) {
    fd = accept();
    if (fd == -1) {
        break;
    }
}
```





#### **2. 传输数据**

![image-20250318223637357](进阶学习/image-20250318223637357.png)

首先理解一下啊，**Application：应用** 以及 **kernel：协议栈**



> **Q: 怎么知道我们有没有发送成功？**
>
> **A:** 有的人会说只要我们的send和write返回-1就代表我们发送成功了，但其实我们的返回-1只代表我们的数据从Application发送到了协议栈上去kernel，存到了我们之前有提到的**wmem**中去。
>
> **很有可能我们从application发送了多次，但是kernel只发送一次**
>
> ---
>
> 这里还会有个概念MTU(最大传输单元)
>
> 这里代表我们的最大传输的大小，例如我们这里可能就是1500



> **Q：这两种写法有区别吗？**
>
> ```c
> recv(fd, buffer, 1000,0);
> 
> for(i = 0;i < 10;i ++) {
>  recv(fd, buffer, 100, 0);
> }
> ```
>
> **A：其实区别不大，正常情况下的话**
>
> 只要我们的缓冲区开辟的够大的话是不影响的



> **Q：传输过程中是怎么保证传输稳定性的**
>
> **A：**通过慢启动和拥塞控制
>
> ![image-20250318224944467](进阶学习/image-20250318224944467.png)
>
> 简单来说是前后都会发送seqnum和acknum，每次通讯的话，两者都是要这样的实现的。
>
> ---
>
> 以下是关于TCP中五个关键机制的详细解释：
>
> 1. **慢启动（Slow Start）**
>
> - **目的**：在连接初始阶段或从拥塞中恢复时，逐步探索可用带宽，避免突然大量数据导致网络拥塞。
> - 机制：
>   - **拥塞窗口（cwnd）**：初始值为1个MSS（最大报文段大小），每收到一个ACK，cwnd指数增长（如翻倍）。
>   - **阈值（ssthresh）**：当cwnd达到阈值时，进入**拥塞避免**阶段，转为线性增长（每RTT增加1个MSS）。
>   - **触发条件**：连接建立时或发生超时重传后启动。
> - **示例**：初始cwnd=1，发送1个报文；收到ACK后cwnd=2，发送2个报文；再收到2个ACK后cwnd=4，依此类推。
>
> ------
>
> 2. **拥塞控制（Congestion Control）**
>
> - **目标**：根据网络拥塞动态调整发送速率，避免网络过载。
> - 核心算法：
>   - **慢启动与拥塞避免**：如上所述，cwnd先指数增长，超过ssthresh后线性增长。
>   - 快速重传与快速恢复（如Reno算法）：
>     - **快速重传**：收到3次重复ACK时立即重传丢失报文，而非等待超时。
>     - **快速恢复**：cwnd减半后进入拥塞避免阶段，避免重置为1。
>   - 处理策略：
>     - **超时重传**：触发慢启动，ssthresh设为当前cwnd的一半，cwnd重置为1。
>     - **重复ACK**：触发快速恢复，cwnd减半后线性增长。
>
> ------
>
> 3. **滑动窗口（Sliding Window）**
>
> - **作用**：实现流量控制，确保发送方不会淹没接收方，同时提高传输效率。
> - 机制：
>   - **接收窗口（rwnd）**：接收方通告剩余缓冲区大小，发送方据此调整发送量。
>   - **实际窗口**：取拥塞窗口（cwnd）和接收窗口（rwnd）的较小值。
>   - **滑动操作**：已确认的数据从窗口中移除，后续数据填入，形成“滑动”效果。
> - **示例**：若窗口大小为4，发送方连续发送4个报文；收到第1个ACK后，窗口滑动，发送第5个报文。
>
> ------
>
> 4. **延迟确认（Delayed Acknowledgment）**
>
> - **目的**：减少ACK报文数量，提升网络效率。
> - 机制：
>   - **延迟策略**：接收方等待最多500ms，或积累两个报文后发送一个ACK。
>   - **捎带确认**：若有数据需发送，ACK可附带在数据包中。
> - **权衡**：可能增加发送方的RTT估计值，导致超时重传延迟，但减少了小包开销。
>
> ------
>
> 5. **超时重传（Retransmission Timeout, RTO）**
>
> - **功能**：确保数据可靠传输，通过重传丢失或损坏的报文。
> - 机制：
>   - **RTT动态计算**：使用加权平均（如EWMA）平滑RTT波动，计算RTO。
>   - **超时处理**：若ACK未在RTO内到达，重传数据并触发拥塞控制（如cwnd重置为1）。
>   - **与快速重传对比**：超时重传反应更强烈（认为严重拥塞），而快速重传基于重复ACK，恢复更快。
>
> ------
>
> **协同工作机制**
>
> - **启动阶段**：慢启动快速提升速率，拥塞控制防止过载。
> - **流量控制**：滑动窗口基于接收方能力调整，拥塞窗口基于网络状况调整。
> - **效率优化**：延迟确认减少ACK数量，滑动窗口允许批量发送；超时重传和快速重传保障可靠性。
> - **拥塞响应**：丢包时，超时重传触发慢启动，重复ACK触发快速恢复，平衡响应速度与网络稳定性。
>
> 通过上述机制，TCP在保证可靠性的同时，最大限度利用网络资源，适应动态变化的网络环境。



#### **3. 断开连接**

> 四次挥手
> 1. 主动方
> 2. 被动方

![image-20250318215749910](进阶学习/image-20250318215749910.png)

![image-20250318230755261](进阶学习/image-20250318230755261.png)

> **四次挥手流程**
>
> 假设客户端主动关闭连接，服务器被动关闭：
> 1. **客户端发送FIN** → 进入`FIN_WAIT_1`状态。  
> 2. **服务器回复ACK** → 进入`CLOSE_WAIT`状态，客户端收到后进入`FIN_WAIT_2`。  
> 3. **服务器发送FIN** → 进入`LAST_ACK`状态。  
> 4. **客户端回复ACK** → 进入`TIME_WAIT`状态，服务器关闭后客户端等待`2MSL`后关闭。
>
> ---
>
> **各状态详解**
>
> 1. **FIN_WAIT_1（客户端）**
>
> - **触发条件**：客户端调用`close()`发送`FIN`报文，请求终止连接。
> - **行为**：  
>   - 等待服务器的`ACK`确认。  
>   - 如果直接收到服务器的`FIN`（极少数情况），说明双方同时关闭，会直接进入`CLOSING`状态。
> - **比喻**：你（客户端）对朋友（服务器）说：“我要挂电话了”（FIN），然后等待对方回应。
>
> ---
>
> 2. **CLOSE_WAIT（服务器）**
>
> - **触发条件**：服务器收到客户端的`FIN`后，回复`ACK`确认。
> - **行为**：  
>   - 服务器进入半关闭状态，仍可向客户端发送未完成的数据。  
>   - 当服务器也准备关闭时，发送自己的`FIN`报文，进入`LAST_ACK`状态。
> - **常见问题**：如果服务器长时间处于`CLOSE_WAIT`，通常是代码未正确调用`close()`，导致资源泄露。  
> - **比喻**：朋友回复：“好的，我知道你要挂了”（ACK），但继续说完自己的话。
>
> ---
>
> 3. **FIN_WAIT_2（客户端）**
>
> - **触发条件**：客户端收到服务器的`ACK`后进入此状态。
> - **行为**：  
>   - 等待服务器发送`FIN`报文。  
>   - 如果长时间未收到`FIN`，可能触发超时（根据系统配置）。
> - **比喻**：你等待朋友说完最后一句话（FIN），然后才能完全挂断。
>
> ---
>
> 4. **LAST_ACK（服务器）**
>
> - **触发条件**：服务器发送自己的`FIN`报文，请求终止连接。
> - **行为**：  
>   - 等待客户端的最后一个`ACK`确认。  
>   - 收到`ACK`后，服务器立即关闭连接。
> - **比喻**：朋友说：“我也说完了，挂了吧”（FIN），等待你的最终确认。
>
> ---
>
> 5. **TIME_WAIT（客户端）**
>
> - **触发条件**：客户端发送最后一个`ACK`后进入此状态。
> - **行为**：  
>   - 等待`2MSL`（Maximum Segment Lifetime，报文最大生存时间，通常30秒~2分钟）。  
>   - **两个作用**：  
>     - 确保最后一个`ACK`能到达服务器（若丢失，服务器会重发`FIN`，客户端可重发`ACK`）。  
>     - 让旧连接的报文在网络中消失，避免与新连接冲突。
> - **常见问题**：高并发服务器可能出现大量`TIME_WAIT`，可通过调整内核参数（如`net.ipv4.tcp_tw_reuse`）优化。  
> - **比喻**：你挂断电话后等2分钟再离开，防止朋友没听到你的“再见”（ACK），需要重说一遍。
>
> ---
>
> 6. **CLOSED（双方）**
>
> - **最终状态**：客户端等待`2MSL`超时后关闭，服务器收到最后一个`ACK`后关闭。
>
> ---
>
> **状态转换图**
>
> ```
> 客户端流程：
> ESTABLISHED → FIN_WAIT_1 → FIN_WAIT_2 → TIME_WAIT → CLOSED
> 
> 服务器流程：
> ESTABLISHED → CLOSE_WAIT → LAST_ACK → CLOSED
> ```
>
> ---
>
> **关键问题解答**
>
> **1. 为什么是四次挥手？**  
>
> - **两次挥手不够**：一方发送`FIN`后，另一方可能还有数据要发送（半关闭状态）。  
> - **三次挥手不够**：双方`FIN`的发送和确认需要独立处理，确保双向数据流完全关闭。
>
> **2. TIME_WAIT为什么需要等待2MSL？**  
>
> - **确保最后一个ACK到达**：若服务器未收到`ACK`，会重发`FIN`，客户端在`2MSL`内可处理。  
> - **清除旧连接报文**：防止相同四元组（源IP、源端口、目标IP、目标端口）的新连接收到旧数据。
>
> **3. 如果最后一次ACK丢失怎么办？**  
>
> - 服务器会重传`FIN`，客户端在`TIME_WAIT`状态下收到后，会**重发ACK**并重置`2MSL`计时器。
>
> ---
>
> **实际场景案例**
>
> #### **场景1：服务器出现大量CLOSE_WAIT**
> - **原因**：服务器未正确调用`close()`关闭连接。  
> - **解决**：检查代码逻辑，确保`FIN`被正确发送（如Java中`socket.close()`）。
>
> #### **场景2：客户端TIME_WAIT过多**
> - **影响**：占用端口资源，可能导致新连接无法建立。  
> - **优化**：  
>   - 开启`tcp_tw_reuse`（允许复用TIME_WAIT连接）。  
>   - 调整`MSL`时间（需谨慎）。
>
> ---
>
> **总结**
>
> 四次挥手通过四个步骤和状态转换，确保了TCP连接的可靠关闭：
> 1. **双向确认**：双方各自发送`FIN`和`ACK`，确认数据完整。  
> 2. **半关闭设计**：允许一方在收到`FIN`后继续发送剩余数据。  
> 3. **容错机制**：`TIME_WAIT`状态防止最后一个`ACK`丢失或旧数据干扰。
>
> 这个过程就像一场礼貌的道别，双方确认彼此都说完话后，才真正挂断电话 📞！




> ==**close**==
>
> fd --> 回收
> fin --> 
>
> ---
>
> - **用法**：`int close(int sockfd);`
> - 原理：
>   - 发送FIN包（TCP四次挥手）
>   - 释放内核资源（缓冲区、套接字结构体）
> - 注意事项：
>   - 可能产生`TIME_WAIT`状态（服务端需处理地址重用）
>   - shutdown()可半关闭连接



> **Q1: ack没有收到，先收到fin**
>
> ![image-20250318231404171](进阶学习/image-20250318231404171.png)
>
> 这种情况的话会走上面的链路，最终的结果是一样的
>
> 
>
> **Q2: 双方同时调用close**
>
> 也就是双方一起发送FIN
>
> ![image-20250318231557037](进阶学习/image-20250318231557037.png)
>
> 这个地方其实可以解释==服务端无缘无故出现大量的time_wait==，但却没有进入关闭。主要可能就是双方一起调用了close的问题







> **Q3: 如果我们不区分客户服务段，如果我们同时发送三次握手会怎么样？什么时候会发生？**
>
> **A:** 用TCP实现P2P
>
> ```c
> fd = socket();
> localaddr, remoteaddr;
> bind( 8000); // optional
> connect();
> ```
>
> 
>
> 
>
> ![image-20250318231515520](进阶学习/image-20250318231515520.png)



```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>

#define MAX_ATTEMPTS 3
#define TIMEOUT_SEC 5

// 非阻塞连接辅助函数
int async_connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen) {
    int flags = fcntl(sockfd, F_GETFL, 0);
    fcntl(sockfd, F_SETFL, flags | O_NONBLOCK);

    int ret = connect(sockfd, addr, addrlen);
    if (ret == 0) {
        fcntl(sockfd, F_SETFL, flags); // 恢复阻塞模式
        return 1; // 立即连接成功
    }

    if (errno != EINPROGRESS) {
        perror("connect failed");
        return -1;
    }

    // 等待连接完成
    fd_set wfds;
    struct timeval tv = {TIMEOUT_SEC, 0};
    FD_ZERO(&wfds);
    FD_SET(sockfd, &wfds);

    ret = select(sockfd+1, NULL, &wfds, NULL, &tv);
    if (ret <= 0) {
        printf("Connection timeout\n");
        return 0;
    }

    // 检查socket错误
    int error = 0;
    socklen_t len = sizeof(error);
    getsockopt(sockfd, SOL_SOCKET, SO_ERROR, &error, &len);
    if (error != 0) {
        errno = error;
        perror("async connect failed");
        return -1;
    }

    fcntl(sockfd, F_SETFL, flags); // 恢复阻塞模式
    return 1;
}

int main(int argc, char *argv[]) {
    if (argc != 4) {
        printf("Usage: %s <local_ip> <local_port> <remote_ip:remote_port>\n", argv[0]);
        return 1;
    }

    // 解析参数
    char *local_ip = argv[1];
    int local_port = atoi(argv[2]);
    
    char remote_ip[INET_ADDRSTRLEN];
    int remote_port;
    sscanf(argv[3], "%[^:]:%d", remote_ip, &remote_port);

    // 创建TCP socket
    int sockfd = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
    if (sockfd < 0) {
        perror("socket creation failed");
        exit(EXIT_FAILURE);
    }

    // 可选绑定本地地址
    struct sockaddr_in local_addr = {
        .sin_family = AF_INET,
        .sin_port = htons(local_port),
        .sin_addr.s_addr = inet_addr(local_ip)
    };
    
    if (bind(sockfd, (struct sockaddr*)&local_addr, sizeof(local_addr)) < 0) {
        perror("bind failed");
        close(sockfd);
        exit(EXIT_FAILURE);
    }

    // 设置目标地址
    struct sockaddr_in remote_addr = {
        .sin_family = AF_INET,
        .sin_port = htons(remote_port),
        .sin_addr.s_addr = inet_addr(remote_ip)
    };

    // 尝试多次连接（应对同时打开）
    for (int attempt = 0; attempt < MAX_ATTEMPTS; attempt++) {
        int result = async_connect(sockfd, (struct sockaddr*)&remote_addr, sizeof(remote_addr));
        
        if (result == 1) {
            printf("Connection established!\n");
            // 进入数据传输阶段...
            break;
        }
        else if (result == -1) {
            close(sockfd);
            exit(EXIT_FAILURE);
        }

        // 等待期间可能收到对方的SYN
        printf("Retrying... (%d/%d)\n", attempt+1, MAX_ATTEMPTS);
        sleep(1);
    }

    close(sockfd);
    return 0;
}
```

**键实现原理说明**

1. **非阻塞连接机制**：
   - 使用`fcntl()`设置非阻塞模式
   - 通过`select()`实现超时控制
   - 检查`SO_ERROR`获取真实错误状态
2. **同时打开处理**：
   - 多次重试连接（模拟双方主动连接）
   - 在等待期间可能接收到对方的SYN包
   - 内核自动完成TCP状态转换（SYN_SENT → SYN_RECV → ESTABLISHED）
3. **端口绑定策略**：
   - 显式绑定固定端口（方便NAT穿透）
   - 使用`SO_REUSEADDR`选项（未展示，需添加）允许端口重用

------

**使用示例**

**终端1**：

bash

```bash
$ ./p2p_tcp 0.0.0.0 12345 192.168.1.100:54321
Retrying... (1/3)
Connection established!
```

**终端2**：

bash

```bash
$ ./p2p_tcp 0.0.0.0 54321 192.168.1.100:12345 
Retrying... (1/3)
Connection established!
```

------

**网络抓包分析（Wireshark示例）**

text

```text
No. Time        Source          Destination     Protocol Info
1   0.000000    192.168.1.100   192.168.1.100   TCP     12345 → 54321 [SYN] 
2   0.000123    192.168.1.100   192.168.1.100   TCP     54321 → 12345 [SYN] 
3   0.000256    192.168.1.100   192.168.1.100   TCP     12345 → 54321 [SYN, ACK] 
4   0.000379    192.168.1.100   192.168.1.100   TCP     54321 → 12345 [SYN, ACK] 
```

> 注意：实际场景中双方IP地址可能不同（这里演示同一主机的环回通信）















# 25年3月20日

## 2.2.3 UDP的可靠性传输协议QUIC

**整体大纲：**

1. UDP如何做到可靠性传输
2. UDP与TCP，我们如何选择
3. UDP如何可靠，KCP协议在哪些方面有优势
4. KCP协议精讲（重点讲解），项目
5. QUIC时代是否已经到来



---



### 一、如何做到可靠性传输

1. ACK机制
2. 重传机制重传策略
3. 序号机制3 2 1-->2 3 1
4. 重排机制2 3 1-->3 2 1
5. 窗口机制、流量控制、带宽有限



### 二、UDP和TCP

#### 2.1 UDP和TCP我们如何选择呢

![image-20250320211544159](进阶学习/image-20250320211544159.png)

##### 1. 基础概念区分

1. **报文传输（如UDP协议）**
   - 特点：每次发送的数据都是一个**独立的数据包**（Datagram）
   - 发送函数：`sendto("hello")` → 直接发送完整报文
   - 接收函数：`recvfrom()` 每次接收一个完整报文
   - 示例：发送两次`sendto("hello")`和`sendto("darren")`，接收端会收到两次独立报文："hello"和"darren"
2. **字节流传输（如TCP协议）**
   - 特点：数据像水管里的流水，**没有明确边界**
   - 发送函数：`send("hello")` → 数据进入发送缓冲区
   - 接收函数：`recv()` 可能一次读取多个发送操作的数据
   - 示例：连续发送两次`send("hello")`和`send("darren")`，接收端可能一次性收到"hellodarren"

##### 2. 核心差异解析（附示意图关键点）

1. **数据边界问题**
   - 报文传输：每个数据包自带边界标识（如示意图中独立箭头）
   - 字节流传输：接收端看到的是一串连续字节（如示意图中的合并箭头）
2. **粘包现象**
   - 图中`recv`可能返回`hellodarren`展示了典型的TCP粘包现象：多次发送的数据被合并接收
   - 图中`recv - h`和`recv - e`演示了另一种可能：单次接收不完整数据（如收到首字母"h"）
3. **数据可靠性**
   - 报文传输：可能出现数据丢失（图中提示"h ello就会丢失"）
   - 字节流传输：通过TCP协议保证数据顺序和完整性



---



#### 2.2 TCP和UDP格式对比

![image-20250320212450667](进阶学习/image-20250320212450667.png)



---

#### 2.3 ARQ协议(Automatic Repeat-reQuest)

> ARQ协议(AutomaticRepeat-reQuest)，即自动重传请求，**是传输层的错误纠正协议之一。**
> 它通过使用确认和超时两个机制，在不可靠的网络上实现可靠的信息传输。
> ARQ协议主要有3种模式：
> (1）即停等式(stop-and-wait)ARQ
> (2）回退n帧(go-back-n)ARQ,
> (3）选择性重传(selectiverepeat)ARQ

##### 2.3.1 ARQ协议-停等式(stop-and-wait)（==一般不用==）

停等协议的工作原理如下:
1、发送方对接收方发送数据包，然后等待接收方回复ACK并且开始计时。
2、在等待过程中，发送方停止发送新的数据包。
3、当数据包没有成功被接收方接收，接收方不会发送ACK.这样发送方在等待一定时间后，重新发送数据包。
4、反复以上步骤直到收到从接收方发送的ACK.

![image-20250320212924349](进阶学习/image-20250320212924349.png)



##### 2.3.2 ARQ协议-回退n帧(go-back-n)

![image-20250320213030191](进阶学习/image-20250320213030191.png)

如上图所示，**序号为2的分组丢失因此分组2及之后的分组都将被重传。**
**总结：GBN采用的技术包括序号、累积确认、检验和以及计时/重传。**





##### 2.3.3 ARQ协议-选择重传(Selective-repeat)

==kcp可靠传输 基于udp==

![image-20250320213156365](进阶学习/image-20250320213156365.png)

**只重传没有确认的数据。**



---

#### 2.4 RTT和RTO

**RTO（RetransmissionTimeOut）**即重传超时时间。（**常见设置为两倍的RTT**）
**RTT(Round-TripTime)：**往返时延。表示从发送端发送数据开始，到发送端收到来自接收端的确认
(接收端收到数据后便立即发送确认），总共经历的时延。

|   **特性**   |        **RTT**         |            **RTO**            |
| :----------: | :--------------------: | :---------------------------: |
|   **目的**   |      测量网络延迟      |       控制重传等待时间        |
|  **动态性**  |   随网络状态实时变化   | 基于 RTT 动态计算（非固定值） |
| **影响场景** | TCP 拥塞控制、QoS 优化 |     重传效率、网络可靠性      |
|  **示例值**  |    40ms（图中示例）    |    通常为 RTT 的 1.5~2 倍     |



---



#### 2.5 流量控制

（1）发送方的速率与接收方的速率是不一定相等。
（2）如果缓存区满了发送方还在疯狂着发送数据，接收方只能把收到的数据包丢掉，大量的丢包会极大着浪费网络资源。
（3）对发送方发送速率的控制，称之为流量控制。

==**UDP要自己控制发送速率**              **tcp不用管**==

![image-20250320213907436](进阶学习/image-20250320213907436.png)

> **流量控制-如何控制？**
>
> 1. 接收方每次收到数据包，可以在发送确定报文的时候，同时告诉发送方自己的缓存区还剩余多少是空闲的。
> 2. 把缓存区的剩余大小称之为接收**窗口**大小，用变量win来表示接收**窗口**的大小。
> 3. 发送方收到之后，便会调整自己的发送速率，也就是调整自己发送窗口的大小
> 4. 当发送方收到接收窗口的大小为0时，发送方就会停止发送数据，防止出现大量丢包情况的发生。![image-20250320214349488](进阶学习/image-20250320214349488.png)

> **发送方何时再继续发送数据？**
> 1.当接收方处理好数据，接受窗口win>0时，接收方发个通知报文去通知发送方。
> 2.当发送方收到接受窗口win=0时，这时发送方停止发送报文，并且同时开启一个定时器，每隔一段时间就发个测试报文去询问接收方，打听是否可以继续发送数据了。**(图示方法)**



> ==**流量控制-小结**==
>
> 1. 接收窗口的大小固定吗？
>    不是固定的，需要根据网络情况动态调整，丢包多的时候需要减小接收窗口。
> 2. 接收窗口越大越好吗？
>    接收窗口过大容易导致丢包
> 3. 发送窗口和接收窗口相等吗？
>    一般接收窗口>=发送窗口。
>
> ![image-20250320214741709](进阶学习/image-20250320214741709.png)



---



#### 2.6 拥塞控制

![image-20250320215124075](进阶学习/image-20250320215124075.png)

拥塞控制和流量控制虽然采取的动作很相似，但拥塞控制与网络的拥堵情况相关联，而流量控制与接收方的缓存状态相关联。
**拥塞控制是根据丢包情况调整发送窗口，调整发送数据的速率。**

---



#### 2.7 UDP并发编程

![image-20250320215549764](进阶学习/image-20250320215549764.png)



---

### 三、 UDP如何可靠，KCP协议在哪些方面有优势

> **以10%-20%带宽浪费的代价换取了比 TCP快30%-40%的传输速度。**
>
> ![image-20250320225327387](进阶学习/image-20250320225327387.png)
>
> **一、核心矛盾点解析**
>
> 用户的问题本质是：**为什么KCP协议声称比TCP快30%-40%，但在网络通畅时TCP的传输速率反而更高？**
> 关键在于理解两种协议的设计目标与适用场景的差异：
>
> |   **特性**   |             **TCP**              |            **KCP（基于UDP）**             |
> | :----------: | :------------------------------: | :---------------------------------------: |
> | **设计目标** | 可靠性优先（保证数据完整、有序） |       速度优先（容忍部分带宽浪费）        |
> | **适用场景** |  稳定网络环境（低丢包、低延迟）  | 高延迟/高丢包网络（如移动网络、跨国传输） |
> | **带宽效率** |        高（拥塞控制优化）        |         较低（主动重传导致冗余）          |
>
> ------
>
> **二、KCP为何更快？——牺牲带宽换速度**
>
> 1. **激进的重传策略**
>
> - **TCP**：采用保守的RTO（超时重传时间）计算，例如首次丢包后等待至少200ms重传。
> - **KCP**：大幅缩短RTO（可设置为10~50ms），**快速重传**丢包数据，减少等待时间。
> - **代价**：频繁重传导致带宽浪费（图中提到的10%-20%）。
>
> 2. **无拥塞控制束缚**
>
> - **TCP**：通过滑动窗口和拥塞控制算法（如Cubic）**限制发送速率**，避免网络过载。
> - **KCP**：不严格限制发送速率，允许更**激进地占用带宽**，尤其在高丢包场景下优势明显。
> - **示例**：在30%丢包率的网络中，KCP可能通过快速重传保持高吞吐，而TCP会因频繁触发拥塞控制降低速率。
>
> 3. **精简的确认机制**
>
> - **TCP**：依赖ACK确认每个数据包，延迟敏感。
> - **KCP**：支持**选择性确认（SACK）** 和 **NACK（否定确认）**，可批量处理丢包信息，减少通信轮次。
>
> ------
>
> **三、为何网络通畅时TCP更快？——带宽利用率的反转**
>
> 1. **TCP的带宽压榨能力**
>
> - 在稳定网络中，TCP的拥塞控制算法会逐渐增大发送窗口，最终**占满可用带宽**。
> - 例如：TCP Cubic算法在无丢包时会指数级扩大窗口，达到物理带宽上限。
>
> 2. **KCP的冗余开销拖累**
>
> - 即使网络通畅，KCP仍会因以下原因浪费带宽：
>   - **预防性冗余**：为应对潜在丢包，可能主动发送重复数据。
>   - **控制报文占比高**：更频繁的ACK/NACK交互占用带宽。
> - **示例**：若物理带宽为100Mbps，TCP可用满100Mbps，而KCP因10%冗余实际有效带宽仅90Mbps。
>
> 3. **协议栈优化差异**
>
> - **TCP**：经过数十年优化，内核层实现效率极高（如TSO/GRO硬件加速）。
> - **KCP**：用户态实现，数据拷贝和协议处理开销较大，难以完全发挥硬件性能。
>
> ------
>
> **四、场景化对比（附表格说明）**
>
> |        **场景**         |       **TCP表现**        |         **KCP表现**          |         **关键原因**         |
> | :---------------------: | :----------------------: | :--------------------------: | :--------------------------: |
> | **网络稳定（0%丢包）**  |    速率高（占满带宽）    |     速率较低（冗余开销）     | TCP无冗余，KCP有固定带宽浪费 |
> |  **高丢包（>5%丢包）**  | 速率骤降（触发拥塞控制） | 速率较高（快速重传弥补丢包） |     KCP抗丢包能力强于TCP     |
> | **高延迟（RTT>200ms）** |  速率受限（窗口增长慢）  |   速率较高（激进发送策略）   |   KCP不严格依赖RTT计算窗口   |
>
> ------
>
> **五、技术选型建议**
>
> 1. **选择TCP的场景**：
>    - 需要最大化带宽利用率（如局域网内文件传输）
>    - 网络质量稳定（企业专线、光纤宽带）
>    - 对传输延迟不敏感（非实时系统）
> 2. **选择KCP的场景**：
>    - 高延迟/高丢包网络（4G/5G移动网络、卫星通信）
>    - 实时性要求高（游戏、视频会议）
>    - 可接受适度带宽浪费（如P2P加速、跨国传输）
>
> ------
>
> **六、总结**
>
> - **KCP的"快"**：本质是**用带宽换时间**，通过牺牲10%-20%带宽换取重传延迟的降低，在恶劣网络中表现优于TCP。
> - **TCP的"慢"**：并非绝对速度慢，而是在高丢包/高延迟场景下，其保守设计导致实际吞吐量下降。
> - **性能反转**：在网络通畅时，TCP的优化机制和零冗余特性使其反超KCP。理解这一矛盾，需结合具体网络条件评估协议选择。



#### 3.1 RTO翻倍vs不翻倍：
**TCP超时计算是RTOx2，这样连续丢三次包就变成RTOx8了，**十分恐怖，而KCP启动快速模式后不x2，只是x1.5（实验证明1.5这个值相对比较好），提高了传输速度。
以RTO=100ms为例：

![image-20250320224609491](进阶学习/image-20250320224609491.png)

> **这边UDP的优势的话**
>
> 主要是因为TCP的RTO增长是写死在协议栈中的，不能被随意改动，否则可能存在不兼容问题（除非将网络断开）
>
> 而使用UDP的话会可以自己设置倍率方便很多



#### 3.2 选择性重传vs全部重传：

TCP丢包时会全部重传从丢的那个包开始以后的数据，KCP是选择性重传，只重传真正丢失的数据包。

==以王者荣耀举例我们可以定制重传策略以及定制丢包策略==

简单来说就是可以丢弃例如两秒前的数据如果发送失败就不需要重发了，只重发当前需要的帧。



#### 3.3 快速重传 (跳过多少个包马上重传)

**（如果使用了快速重传，可以不考虑RTO）**：发送端发送了1,2,3,4,5几个包，然后收到远端的ACK:1,3,4,5，当收到ACK3时，KCP知道2被跳过1次，收到ACK4时，知道2被跳过了2次，此时可以认为2号丢失，不用等超时，直接重传2号包，大大改善了丢包时的传输速度。fastresend=2

> **这边存在带宽浪费问题**
>
> 如果我们只是纯属的慢一点了，待会就传过去了，但是还是默认丢失，直接重传



#### 3.4 延迟ACKVS非延迟ACK：

TCP为了充分利用带宽，延迟发送ACK（NODELAY-针对发送的都没用），这样超时计算会算出较大RTT时间，延长了丢包时的判断过程。KCP的ACK是否延迟发送可以调节。

![image-20250320230143740](进阶学习/image-20250320230143740.png)



#### 3.5 UNAvSACK+UNA:

ARQ模型响应有两种，UNA（此编号前所有包已收到，如TCP）和ACK（该编号包已收到），光用UNA将导致全部重传，光用ACK则丢失成本太高，以往协议都是二选其一，而KCP协议中，除去单独的ACK包外，所有包都有UNA信息。

![image-20250320230436638](进阶学习/image-20250320230436638.png)



#### 3.6 非退让流控：

KCP正常模式同TCP一样使用公平退让法则，即发送窗口大小由：

发送缓存大小、接收端剩余接收缓存大小、丢包退让及慢启动这四要素决定。但传送及时性要求很高的小数据时，可选择通过配置跳过后两步，仅用前两项来控制发送频率。

以牺牲部分公平性及带宽利用率之代价，换取了开着BT都能流畅传输的效果。





### 四、kcp代码测试

#### 4.1 KCP精讲-名词说明

> 用户数据：应用层发送的数据，如一张图片2Kb的数据
>
> - **MTU：**最大传输单元。即每次发送的最大数据，1500。包含头部为1500，实际使用sendto: 1400
> - **RTO：**RetransmissionTimeOut，重传超时时间。
> - **cwnd: **congestionwindow，拥塞窗口，表示发送方可发送多少个kCP数据包。与接收方窗口有关，与网络状况（拥塞控制）有关，与发送窗口大小有关。
> - **rwnd: **receiverwindow,接收方窗口大小，表示接收方还可接收多少个kCP数据包
> - **snd_queue: **待发送kCP数据包队列
> - snd_buf:
> - **snd_nxt:** 下一个即将发送的kcp数据包序列号
> - **snd_una: **下一个待确认的序列号，即是之前的包接收端都已经收到。



#### 4.2 kcp使用方式

![image-20250321001443414](进阶学习/image-20250321001443414.png)

![image-20250321001832162](进阶学习/image-20250321001832162.png)

> ==**kcp聊天室流程**==
>
> 1. 创建KCP对象：ikcpcb *kcp = ikcp_create(conv,user);
>
> 2. 设置发送回调函数（如UDP的send函数）：kcp->output=udp_output;
>
>    真正发送数据需要调用sendto
>
> 3. 循环调用 update：ikcp_update(kcp,millisec); //在一个线程、定时器 5ms/10m做调度
>
> 4. 输入一个应用层数据包(如UDP收到的数据包）：ikcp_input(kcp,received_udp_packet,received_udp_size);
>
>    我们要使用recvfrom接收，然后扔到kcp里面做解析
>
> 5. 发送数据：ikcp_send(kcp1,buffer,8);用户层接口
>
> 6. 接收数据：hr=ikcp_recv(kcp2,buffer,10);用户层读取数据



> **==不同客户端如果生成相同id怎么办==**
>
> 1. 每个通道 客户端-服务端 kcp 是一一对应
>     conv由客户端产生，会不会导致一样的会话id?
>     确实有可能
>     uuid的算法 -生成唯一的conv id
>
> 课上为了简单：客户端使用随机数产生的conv 会话id
>
> 2.先使用http 向服务端请求 获取conv id





#### 4.3 kcp源码流程图

![image-20250321003634587](进阶学习/image-20250321003634587.png)

![image-20250321004328697](进阶学习/image-20250321004328697.png)

##### **4.3.1 整体架构与流程概览**

1. **核心模块划分**

- **数据接收逻辑**：处理UDP接收的数据包，解包并提交给应用层。
- **数据发送逻辑**：将应用层数据编码为KCP协议包，通过UDP发送。
- **双向交互**：通过**确认包（ACK）**实现发送端与接收端的动态反馈。

2. **关键函数与操作**

|   函数/操作    |                作用                |    所属逻辑     |
| :------------: | :--------------------------------: | :-------------: |
| `kcp_create()` | 创建KCP实例，初始化窗口、超时参数  | 发送/接收初始化 |
| `kcp_input()`  | 处理接收到的UDP数据包（协议解析）  |  数据接收逻辑   |
|  `kcp_recv()`  |    提取已排序的有效数据给应用层    |  数据接收逻辑   |
|  `kcp_send()`  |     将应用数据封装为KCP协议包      |  数据发送逻辑   |
| `kcp_update()` |    定时触发重传、窗口更新等事件    |  数据发送逻辑   |
|   `output()`   | 通过UDP发送KCP协议包（用户自定义） |  数据发送逻辑   |

------

##### 4.3.2数据接收逻辑详解

1. **流程步骤**

```plaintext
UDP收数据 → kcp_input() → 解包、更新确认 → kcp_recv() → 应用层
```

2. **核心机制解析**

- **`kcp_input()`**
  - **功能**：解析UDP数据包，提取KCP协议头（包含序列号、确认号、窗口信息等）。
  - 关键操作：
    1. **处理ACK**：根据接收到的确认号，更新发送端的已确认数据包列表。
    2. **处理数据包**：若为新的有效数据包，存入接收缓冲区，并生成ACK。
    3. **乱序处理**：若数据包非连续到达，暂存待后续排序（与TCP不同，KCP允许选择性确认）。
- **解包与更新确认**
  - **解包**：剥离KCP协议头，还原原始数据负载。
  - **更新发送确认**：通过`ikcp_ack_push`函数将ACK信息加入发送队列（后续通过`output()`发送）。
- **`kcp_recv()`**
  - **功能**：从接收缓冲区中提取连续且有序的数据提交给应用层。
  - **排序逻辑**：若缓冲区中存在序列号连续的数据块，立即提交；否则等待缺失包到达（类似TCP的滑动窗口）。

3. **示例场景**

- **正常接收**：收到序列号5的包且之前已收到1-4，则立即提交给应用层。
- **乱序接收**：收到序列号7的包，但6未到达，则暂存缓冲区，等待重传请求触发。

------

##### 4.3.3 数据发送逻辑详解

1. **流程步骤**

```plaintext
应用层 → kcp_send() → 编包 → kcp_update() → 调用output() → UDP发送
```

2. **核心机制解析**

- **`kcp_send()`**
  - **功能**：将应用层数据分割为适合传输的KCP数据包（根据MTU大小）。
  - **协议封装**：添加KCP协议头（含序列号、时间戳、窗口信息），加入发送缓冲区。
- **编包与发送确认包**
  - **动态编包**：每次`kcp_update()`触发时，检查发送缓冲区中的待发数据。
  - **确认包发送**：若存在待发送的ACK信息（来自接收逻辑），优先封装为ACK包发送。
- **`kcp_update()`**
  - **定时驱动**：通常由用户设置定时器（如每10ms调用一次）。
  - **关键操作：**
    1. **超时重传**：检查未确认的数据包是否超过RTO（超时重传时间），触发重传。
    2. **窗口控制**：根据接收端反馈的窗口大小，调整发送速率。
    3. **流量控制**：若网络拥塞（如连续丢包），减少发送窗口，避免恶化网络状态。
- **`output()`**
  - **用户自定义实现**：将KCP协议包通过UDP发送（如调用`sendto()`函数）。
  - **带宽优化**：合并多个小包为单个UDP包发送（减少协议头开销）。

3. **示例场景**

- **首次发送**：应用调用`kcp_send(data)`，数据被分割为序列号1-3的包，加入发送队列。
- **触发重传**：若序列号2的包在RTO时间内未收到ACK，`kcp_update()`会将其重新加入发送队列。
- **动态窗口调整**：接收端反馈窗口已满，发送端暂停发送新数据，直至窗口更新。







#### 4.4 kcp的配置模式

1. **工作模式：**intikcp_nodelay(ikcpcb *kcp,int nodelay,int interval,int resend,int nc)

   - nodelay：是否启用nodelay模式，0不启用；1启用。

   - interval：协议内部工作的interval，单位毫秒，比如10ms或者20ms

   - resend：快速重传模式，默认o关闭，可以设置2(2次AcK跨越将会直接重传)

   - nc：是否关闭流控，默认是0代表不关闭，1代表关闭。

     

   - 默认模式：ikcp_nodelay(kcp,0,10,0,0);

   - 普通模式：ikcp_nodelay(kcp,0,10,0,1);关闭流控等

   - 极速模式：ikcp_nodelay(kcp,2,10,2,1)，并且修改kcp1->rX_minrto=10；

     ​									kcp1->fastresend = 1;

2. **最大窗口：**intikcp_wndsize(ikcpcb *kcp,intsndwnd,intrcvwnd);
   该调用将会设置协议的最大发送窗口和最大接收窗口大小，默认为32，单位为包。
3. **最大传输单元：**intikcp_setmtu(ikcpcb*kcp,intmtu);
   kcp协议并不负责探测MTU，默认mtu是1400字节
4. **最小RTO：**不管是TCP还是KCP计算RTO时都有**最小RTO的限制**，即便计算出来RTO为40ms，由于默认的RT0是100ms，协议只有在100ms后才能检测到丢包，快速模式下为30ms，可以手动更改该值：kcp->rx_minrto=10;



#### 4.5 kcp协议头

![image-20250321212605701](进阶学习/image-20250321212605701.png)

![image-20250321212731035](进阶学习/image-20250321212731035.png)

下面这几个是[4]中常用的指令

![image-20250321212755939](进阶学习/image-20250321212755939.png)4

**1. conv（连接号）**

- **位置**：0-3字节
- **作用**：UDP本身是无连接的，`conv`字段用于标识客户端连接。类似于TCP的“虚拟连接”，确保数据包归属于同一会话。
- **应用场景**：服务端通过`conv`区分不同客户端的请求，实现多路复用。

------

**2. cmd（命令字）**

- **位置**：第4字节
- 作用：控制数据包类型，包含三种核心命令：
  - **IKCP_CMD_ACK**：确认包（Acknowledgment），告知对方已收到数据。
  - **IKCP_CMD_WASK**：接收窗口探测，主动询问接收方的剩余窗口大小。
  - **IKCP_CMD_WINS**：接收窗口通知，主动告知对方当前可用窗口大小。
- **协议特性**：通过命令字实现灵活的流量控制和可靠性机制，而非依赖底层协议。

------

**3. frg（分片标识）**

- **位置**：第5字节
- 作用：标识用户数据是否分片及分片顺序。
  - **报文模式**：分片序号递减（如3、2、1、0），接收方按顺序重组。
  - **流模式**：所有分片标记为0，适用于无需严格顺序的场景（如实时音视频）。
- **分片逻辑**：若用户数据超过单个包容量（1376字节），KCP自动分片发送。

------

**4. wnd（接收窗口大小）**

- **位置**：6-7字节
- **作用**：接收方当前可接收的数据量（单位：包）。发送方根据此值动态调整发送速率，避免拥塞。
- **流量控制**：类似TCP的滑动窗口，但KCP允许更灵活的窗口调整策略，减少延迟。

------

**5. ts（时间戳）**

- **位置**：8-11字节
- **作用**：记录数据包发送时间，用于计算RTT（往返时间）和判断超时重传。
- **重传机制**：结合`sn`序列号，快速检测丢包并触发重传，优于TCP的延迟ACK机制。

------

**6. sn（序列号）**

- **位置**：12-15字节
- **作用**：唯一标识数据包的发送顺序，接收方按`sn`确认数据完整性。
- **协议特性**：KCP通过连续`sn`实现数据有序性，支持选择性重传（仅重传丢失的包）。

------

**7. una（下一个待接收序号）**

- **位置**：16-19字节
- **作用**：类似TCP的确认号（ACK），表示接收方期望收到的下一个包序号。例如，收到`sn=10`的包后，`una=11`。
- **快速确认**：通过`una`可批量确认此前所有包，减少ACK包数量，提升效率。

------

**8. len（数据长度）**

- **位置**：20-23字节
- **作用**：指示可选数据部分（`data`字段）的实际长度，最大为1376字节（总包大小1400字节 - 24字节头部）。
- **分片关联**：若数据被分片，每个分片的`len`为该分片的数据长度。

------

**9. data（用户数据）**

- **位置**：24字节之后
- **作用**：承载实际传输的数据。
- **分片规则**：数据总长度超过1376字节时，按`frg`字段分片发送，接收端重组。

---



#### 4.6 kcp数据发送过程

![image-20250321215342977](进阶学习/image-20250321215342977.png)

我们需要有个发送缓存区





![image-20250321223633518](进阶学习/image-20250321223633518.png)

> **snd_wnd：**固定大小，默认32
> **rmt_wnd：**远端接收窗口大小，默认32
> **cwnd：**滑动窗口，可变，越小一次能发送的数据越小
>
> 发送速率的控制是：本质是根据滑动窗口，控制把数据从snd__queue加入到send_buf。
>
> 调用void ikcp_flush(ikcpcb*kcp)发送数据
>
> **IKCP_CMD_PUSH** 命令正常的数据
> **CMD_ACK**应答







#### 4.7 kcp接收数据过程

![image-20250321224633228](进阶学习/image-20250321224633228.png)



![image-20250321231301242](进阶学习/image-20250321231301242.png)

#### 4.8 kcp确认包处理流程

![image-20250321231813769](进阶学习/image-20250321231813769.png)



#### 4.9 kcp快速确认

![image-20250321232121671](进阶学习/image-20250321232121671.png)





#### 4.10 ikcp_input逻辑

![image-20250321233204957](进阶学习/image-20250321233204957.png)





#### 4.11 应答列表acklist

![image-20250321233338314](进阶学习/image-20250321233338314.png)

![image-20250321233342161](进阶学习/image-20250321233342161.png)





#### 4.12 流量控制和拥塞控制

![image-20250321233540704](进阶学习/image-20250321233540704.png)

#### 4.13 探测对方接收窗口

![image-20250321233640631](进阶学习/image-20250321233640631.png)





#### 4.14 如何在项目中集成kcp（一）

![image-20250321233741403](进阶学习/image-20250321233741403.png)



#### 4.15 如何在项目中集成kcp（二）

> **UdpSocket核心：**
>
> - **Bind:** 服务端时绑定端口
> - **RecvFrom：**接收数据udp数据
> - **SendTo：**发送数据udp数据
>
> **KcpSession核心：**
>
> - **Update：**调度kcp（ikcp_udpate）和发送心跳包（客户端）
> - **Input:**接收recvfrom获取到的udp数据（不是内部调用recvfrom)
> - **Send:**发送用户数据，注意内部没有直接调用sendto
> - **Recv：**用户读取拼接好的用户数据
> - **sendTo：**通过udp通道发送数据，本质是调用sendto
> - **conv：**获取kcp的conv会话id
> - **SetKeepAlive：**设置当前session最新的心跳时间（服务端)
>
> **KcpClient核心-客户端封装：**
>
> - **KcpClient：**服务端ip、port、kcp_opt参数
> - **Run：**需要跑在线程里，处理kcp的调度和收发数据
> - **Send：**发送用户数据
> - **HandleMessage：**处理服务端发过来的用户数据
> - **CheckRet：**判断recvform的返回值
>
> **KcpServer核心-服务端封装：**
>
> - **KcpServer：**监听的ip、port，kcp_opt参数
> - **Run:**内部自己创建线程，处理kcp的调度和收发数据
> - **Update:**kcp调度相关
> - **HandleSession：**处理客户端数据
> - **Notify：**把客户端发送过来的用户数据发给每个客户端



#### 4.16 如何在项目中集成kcp（三）

![image-20250322015642643](进阶学习/image-20250322015642643.png)







### 五、QUIC协议

![image-20250322015857595](进阶学习/image-20250322015857595.png)

**![image-20250322015954277](进阶学习/image-20250322015954277.png)**

**==可以有多个逻辑通道==**





### 六、作业

![image-20250322020144629](进阶学习/image-20250322020144629.png)











# 2.3 自研协程框架

## 2.3.1 协程设计原理与汇编实现（ntyco开源项目）

首先是引入部分！！！

> 1. 为什么会要有协程？
> 2. 协程实现过程，原语操作（哪些原语操作）**不可以被分割的操作就是原语操作**
> 3. 协程如何定义 struct coroutine
> 4. 调度器如何定义， struct scheduler
> 5. 调度器的执行策略
> 6. posix api做到一致
> 7. 协程的执行流程
> 8. 协程的多核模式
> 9. 协程的性能如何测试——100w并发



> ==**需要仿写代码！！！再结合视频！！！！**==
>
> 1. 听完以后，不知道重点在哪里？
> 2. 自己方式实现完以后，不知道如何用？
>
> 学完之后可以用在：
>
> 1. webserver协程做为网络库，
> 2. kv存储，redis
> 3. 图床 网络层





### 一、为什么要有协程

以一个例子来看，我们对我们的网址进行解析！！！！

#### 1.1 dns解析客户端

**实现基于UDP协议的高性能异步DNS解析客户端！**

```mermaid
graph LR
    A[Main Thread] -->|提交请求| B(Async Context)
    B --> C[EPOLL Instance]
    C --> D[[Worker Thread]]
    D -->|事件就绪| E[DNS Server]
    E -->|响应数据| F[Callback]
```

![image-20250323230241861](进阶学习/image-20250323230241861.png)

首先是主函数！！！

![image-20250323230141061](进阶学习/image-20250323230141061.png)

这边分为同步和异步两种写法，分析区别！！！

**1. 同步阻塞模式（#if 1）**

```c
for (i = 0;i < count;i ++) {
    dns_client_commit(domain[i]); // 顺序执行每个DNS查询
}
```

运行特征：

- **单线程串行执行**：逐个处理域名解析

- **同步阻塞I/O**：每个recvfrom调用都会阻塞线程

- **无并发能力**：总耗时 = Σ单次查询耗时

- **资源占用低**：无需维护上下文状态

- **调试友好**：执行流程简单直观

  

**2. 异步非阻塞模式（#else）**

```c
struct async_context *ctx = dns_async_client_init();
for (i = 0;i < count;i ++) {
    dns_async_client_commit(ctx, ...); // 批量提交请求
}
getchar(); // 保持主线程存活
```

运行特征：

- **事件驱动架构**：通过epoll管理所有socket
- **非阻塞I/O**：设置O_NONBLOCK标志
- **多路复用机制**：单线程处理数千并发请求
- **回调通知机制**：通过dns_async_client_result_callback异步返回结果
- **高并发低延迟**：总耗时 ≈ 最慢单次查询耗时



**3. 关键差异对照表**

|   特性    |      同步模式      |        异步模式        |
| :-------: | :----------------: | :--------------------: |
| 线程模型  |     单线程串行     |    主线程+工作线程     |
|  I/O方式  |       阻塞型       |      非阻塞+epoll      |
| 内存消耗  |    低（约1MB）     | 较高（约10MB/万连接）  |
| CPU利用率 | 低（主要等待I/O）  | 高（事件循环密集计算） |
| 适合场景  | 少量请求、简单调试 |     高并发生产环境     |
|  吞吐量   |      数十QPS       |        万级QPS         |

![image-20250323232649468](进阶学习/image-20250323232649468.png)





#### 1.2 之前百万并发测试

![image-20250323233208617](进阶学习/image-20250323233208617.png)

![image-20250323233158960](进阶学习/image-20250323233158960.png)

![image-20250324015025990](进阶学习/image-20250324015025990.png)





![image-20250329232427915](进阶学习/image-20250329232427915.png)



#### 1.3 异步的优点和缺点

优点的话主要是：

	1. 性能高

缺点的话：

1. 主要是不好理解，以下图举例：如果我们的回调函数中再次嵌套回调调用的话，就会产生很多的理解困难。



#### 1.4 为什么要有协程

**同步的编程思路，异步的性能**

>  互联网产品中存在的同时有多个请求存在的功能
>
> 1. **社交媒体动态加载（微博/Twitter）**
>
> - **同步**：用户下拉刷新动态时，实时加载最新内容（等待服务端响应）。
> - **异步**：后台预加载用户未读的评论、点赞通知（非阻塞）。
> - **协程**：单线程内并发处理数万用户动态的渲染和分页逻辑。
> - **技术原理**：动态流使用协程批量加载数据，用户交互操作（如点赞）通过异步队列处理，实时刷新界面需同步确认结果。
>
> ------
>
> 2. **网盘文件上传（Google Drive/百度网盘）**
>
> - **同步**：显示实时上传进度条（需即时反馈）。
> - **异步**：后台分片上传文件到分布式存储（非阻塞IO）。
> - **协程**：同时管理多个文件分片的上传任务（如100个分片并发）。
> - **技术原理**：文件分片后通过协程批量提交到异步队列，进度更新通过同步回调触发前端渲染。
>
> ------
>
> 3. **在线文档协作（腾讯文档/Google Docs）**
>
> - **同步**：用户输入时实时同步内容到本地内存。
> - **异步**：将变更通过消息队列推送到其他协作用户。
> - **协程**：单连接处理多人协同的编辑冲突合并（如OT算法）。
> - **技术原理**：用户输入事件通过WebSocket同步到服务端，服务端用协程处理冲突，异步广播给其他客户端。
>
> ------
>
> 4. **直播弹幕系统（B站/Twitch）**
>
> - **同步**：用户发送弹幕时立即显示在本机（本地渲染）。
> - **异步**：弹幕消息批量推送到所有观众客户端（减少高频IO压力）。
> - **协程**：单服务节点处理数万条弹幕的过滤、合并和分发。
> - **技术原理**：弹幕先通过协程合并压缩，再通过异步消息队列广播，本地渲染需同步保证低延迟。
>
> ------
>
> 5. **打车派单系统（Uber/滴滴）**
>
> - **同步**：用户点击叫车后实时等待司机接单（短时阻塞）。
> - **异步**：后台计算附近司机并批量推送订单（高延迟操作）。
> - **协程**：并行匹配乘客需求与司机画像（如路线/评分）。
> - **技术原理**：派单算法通过协程处理多维度数据，同步返回接单状态，司机响应通过异步事件触发。





### 二、协程如何实现

![image-20250330000720546](进阶学习/image-20250330000720546.png)



下面这部分就是协程的实现，通过函数之间的跳转来执行一些异步操作。

![image-20250330000739063](进阶学习/image-20250330000739063.png)



#### 1. **函数结构与调用关系**
这张图描述了一个基于**异步非阻塞IO**和**协程切换**的编程模型，核心是通过多种跳转方式实现函数间的非顺序执行。以下是图中函数的逻辑分层：

| 函数名        | 核心操作                       | 作用                                             |
| ------------- | ------------------------------ | ------------------------------------------------ |
| `func()`      | `setjmp()` + `async_send/recv` | 入口函数，初始化协程上下文，发起异步IO操作       |
| `func()`      | `async_send/recv`              | 子协程逻辑，处理IO的中间阶段                     |
| `func()`      | `async_send/recv`              | 子协程逻辑，处理IO的最终阶段                     |
| `func()`      | `send()/recv()`                | 传统同步IO操作（可能作为对比存在）               |
| `async_xxx()` | `poll(fd, 0)` + `switch()`     | 异步IO事件循环，检测文件描述符状态并触发协程切换 |

#### 2. **关键流程**
```plaintext
用户调用入口函数(func)
  → setjmp保存当前上下文
  → async_send发起异步发送
  → 进入异步事件循环(async_xxx)
    → poll检测到fd就绪
    → switch跳转到对应回调
      → 处理接收数据(func)
      → async_recv继续异步操作
```



---

#### 3. 跳转方式对比与应用场景

| 方式       | 性能 | 安全性 | 可移植性 | 典型应用场景            |
| ---------- | ---- | ------ | -------- | ----------------------- |
| `setjmp`   | 中   | 低     | 高       | 简单异常处理            |
| `ucontext` | 低   | 中     | 中       | 教学用协程实现          |
| 手写汇编   | 高   | 高     | 低       | 高性能协程库（如libco） |



#### 4. 使用setjmp和longjmp

![image-20250330002356685](进阶学习/image-20250330002356685-1743265442382-1.png)

代码执行流程分析

假设程序按以下顺序执行（参考代码逻辑）：

```plaintext
1. main() 首次调用 setjmp(env) → 返回 0
   → 进入 if (ret == 0) 分支
   → 调用 func(0)

2. func(0) 内部执行：
   - 打印 "func = 0"
   - 调用 longjmp(env, ++arg) → arg=1，跳转回 main() 的 setjmp 处
   → 此时 setjmp 返回 1

3. main() 处理返回值为 1：
   → 进入 else if (ret == 1) 分支
   → 调用 func(1)

4. func(1) 内部执行：
   - 打印 "func = 1"
   - 调用 longjmp(env, ++arg) → arg=2，跳转回 main() 的 setjmp 处
   → 此时 setjmp 返回 2

5. main() 处理返回值为 2：
   → 进入 else if (ret == 2) 分支
   → 调用 func(2)

6. func(2) 内部执行：
   - 打印 "func = 2"
   - 调用 longjmp(env, ++arg) → arg=3，跳转回 main() 的 setjmp 处
   → 此时 setjmp 返回 3

7. main() 处理返回值为 3：
   → 进入 else if (ret == 3) 分支
   → 调用 func(3)

8. func(3) 内部执行：
   - 打印 "func = 3"
   - 调用 longjmp(env, ++arg) → arg=4，跳转回 main() 的 setjmp 处
   → 此时 setjmp 返回 4

9. main() 发现没有处理 ret=4 的条件分支 → 程序结束
```



#### 5. 使用`ucontext`

![image-20250330020731095](进阶学习/image-20250330020731095.png)

```c
/* 编译命令：gcc -o context_switch context_switch.c -Wall -Wextra -ldl */

// 必须定义_GNU_SOURCE宏以使用ucontext相关函数
#define _GNU_SOURCE
#include <stdio.h>
#include <ucontext.h>  // 协程上下文操作库

ucontext_t ctx[3];     // 定义三个协程上下文
ucontext_t main_ctx;   // 主程序上下文
int count = 0;         // 共享计数器（存在竞态条件风险，需原子操作或锁保护）

/*----------------------------------------------------------
  协程1函数：循环打印1和4，切换至协程2
----------------------------------------------------------*/
void fun1() {
    while(count++ < 30) {  // 竞态条件：多个协程共享修改count
        printf("1\n");
        swapcontext(&ctx[0], &ctx[1]); // 保存当前上下文，切换到协程2
        printf("4\n");                 // 恢复执行时继续此处
    }
}

/*----------------------------------------------------------
  协程2函数：循环打印2和5，切换至协程3
----------------------------------------------------------*/
void fun2() {
    while(count++ < 30) {
        printf("2\n");
        swapcontext(&ctx[1], &ctx[2]); // 切换到协程3
        printf("5\n");
    }
}

/*----------------------------------------------------------
  协程3函数：循环打印3和6，切换至协程1
----------------------------------------------------------*/
void fun3() {
    while(count++ < 30) {
        printf("3\n");
        swapcontext(&ctx[2], &ctx[0]); // 切换回协程1
        printf("6\n");
    }
}

/*----------------------------------------------------------
  主函数（初始化协程并启动调度）
----------------------------------------------------------*/
int main() {
    // 为每个协程分配独立的栈空间（已修复栈共享问题）
    char stack1[2048] = {0};
    char stack2[2048] = {0};
    char stack3[2048] = {0};

    // 初始化协程1上下文
    getcontext(&ctx[0]);
    ctx[0].uc_stack.ss_size = sizeof(stack1);
    ctx[0].uc_stack.ss_sp = stack1;
    ctx[0].uc_link = &main_ctx;  // 协程结束后返回主程序
    makecontext(&ctx[0], fun1, 0);

    // 初始化协程2上下文
    getcontext(&ctx[1]);
    ctx[1].uc_stack.ss_size = sizeof(stack2);
    ctx[1].uc_stack.ss_sp = stack2;
    ctx[1].uc_link = &main_ctx;
    makecontext(&ctx[1], fun2, 0);

    // 初始化协程3上下文
    getcontext(&ctx[2]);
    ctx[2].uc_stack.ss_size = sizeof(stack3);
    ctx[2].uc_stack.ss_sp = stack3;
    ctx[2].uc_link = &main_ctx;
    makecontext(&ctx[2], fun3, 0);

    // 启动调度：切换到协程1
    printf("swapcontext\n");
    swapcontext(&main_ctx, &ctx[0]); 

    // 所有协程结束后回到此处
    printf("\n");
    return 0;
}
```



![image-20250330142656840](进阶学习/image-20250330142656840.png)

> 这边其实是为了解答我个人的一个疑惑：
>
> ==**这个A1->B1->C1->A2->B2->C2->A1->B1->C1这样的操作与我妈执行A12->B12->C12->A12->B12->C12又有什么本质上的区别呢**==
>
> ------
>
> ### **协程与同步代码的本质区别：并发能力与资源利用率**
>
> 你的观察非常敏锐！从表面执行顺序看，协程链式切换（A1→B1→C1→A2→B2→C2...）和同步循环（A12→B12→C12→A12→B12...）似乎都按固定顺序执行任务。但它们的核心区别在于 **执行流的控制权分配** 和 **阻塞操作的响应方式**。以下通过具体场景说明差异：
>
> ------
>
> ### **一、同步代码的局限性**
>
> #### **场景：处理三个网络请求**
>
> python
>
> ```python
> # 同步代码（伪代码）
> def main():
>     while True:
>         request_A()  # 阻塞等待A完成
>         request_B()  # 阻塞等待B完成
>         request_C()  # 阻塞等待C完成
> ```
>
> - 问题：
>   - 每个请求必须**顺序执行**，若每个请求耗时 1 秒，完成一轮需要 3 秒。
>   - CPU 在等待网络响应时完全空闲，资源利用率低。
>
> ------
>
> ### **二、协程的并发优势**
>
> #### **场景：同一任务用协程实现**
>
> ```python
> # 协程代码（伪代码）
> async def request_A():
>     await send_request()  # 非阻塞发送请求
>     yield_to_scheduler()  # 主动让出CPU
> 
> async def request_B():
>     await send_request()
>     yield_to_scheduler()
> 
> async def request_C():
>     await send_request()
>     yield_to_scheduler()
> 
> def scheduler():
>     tasks = [request_A(), request_B(), request_C()]
>     while tasks:
>         for task in tasks:
>             resume(task)  # 轮询恢复任务
> ```
>
> - 优势：
>   - 三个请求**并发发送**，总耗时 ≈ 1 秒（假设网络延迟为 1 秒）。
>   - CPU 在等待时切换处理其他任务，利用率接近 100%。
>
> ------
>
> ### **三、关键区别分析**
>
> |     **特性**     |             **协程**             |         **同步代码**         |
> | :--------------: | :------------------------------: | :--------------------------: |
> |  **执行控制权**  |     任务主动让出（非抢占式）     | 函数完全执行完毕才释放控制权 |
> | **阻塞操作处理** |    通过挂起（Yield）避免阻塞     |       必须等待操作完成       |
> |   **并发能力**   |     单线程内并发处理多个任务     |    无法并发，严格顺序执行    |
> |   **适用场景**   | I/O 密集型（网络、磁盘、数据库） |         简单线性任务         |
> |   **资源开销**   |      极低（协程栈约 KB 级）      |          无额外开销          |
>
> ------
>
> ### **四、你的代码问题定位**
>
> #### **当前代码的协程实现**
>
> ```c
> // 协程函数示例
> void func1() {
>     while (count++ < 30) {
>         printf("1\n");
>         swapcontext(&ctx[0], &main_ctx); // 切换回主循环
>         printf("4\n");
>     }
> }
> ```
>
> - 缺陷：
>   1. **无实际阻塞操作**：协程在纯计算中切换不会提升性能。
>   2. **强制返回到主循环**：形成串行链，无法实现并发。
>   3. **共享变量竞态**：`count` 的自增操作未同步。
>
> #### **改进方向**
>
> 1. 模拟非阻塞 I/O：
>
>    ```c
>    void func1() {
>        while (1) {
>            // 1. 发起非阻塞网络请求
>            printf("A: Request sent\n");
>            swapcontext(&ctx[0], &scheduler_ctx); // 让出 CPU
>    
>            // 3. 收到响应后恢复执行
>            printf("A: Response received\n");
>        }
>    }
>    ```
>
> 2. 事件驱动调度器：
>
>    ```c
>    void scheduler() {
>        while (1) {
>            // 2. 检查哪些协程的 I/O 已完成
>            if (request_A_ready) swapcontext(&scheduler_ctx, &ctx[0]);
>            if (request_B_ready) swapcontext(&scheduler_ctx, &ctx[1]);
>        }
>    }
>    ```
>
> ------
>
> ### **五、协程的核心价值总结**
>
> - **并发性**：通过非阻塞 I/O 和协作式切换，单线程内实现高并发。
> - **资源效率**：避免线程/进程的创建和切换开销。
> - **代码可读性**：用同步写法实现异步逻辑（如 `async/await` 语法）。
>
> ------
>
> ### **六、何时使用协程？**
>
> - **适合场景**：
>   - 高并发网络服务（Web 服务器、API 网关）。
>   - 频繁 I/O 操作（数据库查询、文件读写）。
>   - 需要轻量级任务管理（游戏逻辑、爬虫）。
> - **不适合场景**：
>   - 纯 CPU 密集型计算（需用多线程/进程利用多核）。
>   - 简单线性任务（协程反而增加复杂度）。
>
> ------
>
> 通过将协程与 **非阻塞 I/O** 和 **事件循环** 结合，才能真正发挥其威力。理解这一点后，你会发现协程的调度逻辑远不止简单的顺序切换！





### 三、用Hook实现一个Tcp

```mermaid
graph TD
    A[启动服务器] --> B[创建非阻塞 Socket]
    B --> C[绑定端口 2048]
    C --> D[监听连接]
    D --> E{Accept 连接}
    E --> |有新连接| F[接受客户端连接]
    E --> |无连接| D
    F --> G[设置客户端非阻塞]
    G --> H{读取数据}
    H --> |数据就绪| I[Hook read 记录日志]
    I --> J[回显数据]
    J --> H
    H --> |数据未就绪| K[切换协程]
    K --> H
    H --> |客户端断开| L[关闭连接]
    L --> D
```

```c
#define _GNU_SOURCE  // 启用 GNU 扩展
#include <dlfcn.h>   // 动态链接库操作（dlsym）
#include <stdio.h>
#include <ucontext.h> // 协程上下文操作
#include <string.h>
#include <unistd.h>
#include <fcntl.h>    // 文件控制（fcntl）
#include <sys/socket.h>
#include <errno.h>
#include <netinet/in.h>
#include <sys/poll.h>

// ===================== Hook 机制 =====================
typedef ssize_t (*read_t)(int fd, void *buf, size_t count);
read_t read_f = NULL;  // 原始 read 函数指针

typedef ssize_t (*write_t)(int fd, const void *buf, size_t count);
write_t write_f = NULL; // 原始 write 函数指针

// ----------------- Hook 的 read 函数 -----------------
ssize_t read(int fd, void *buf, size_t count) {
    struct pollfd fds[1] = { {.fd = fd, .events = POLLIN} };
    int res = poll(fds, 1, 0); // 非阻塞检查可读性

    if (res <= 0) { // 数据未就绪，切换协程
        ucontext_t *next = &co_ctx; // 假设存在其他协程上下文
        swapcontext(&main_ctx, next); // 切换上下文
    }

    ssize_t ret = read_f(fd, buf, count); // 调用原始 read
    if (ret > 0) printf("Hook read: %s\n", (char*)buf);
    return ret;
}

// ----------------- Hook 的 write 函数 ----------------
ssize_t write(int fd, const void *buf, size_t count) {
    printf("Hook write: %s\n", (const char*)buf);
    return write_f(fd, buf, count); // 调用原始 write
}

// ----------------- 初始化 Hook ------------------------
void init_hook() {
    if (!read_f) read_f = dlsym(RTLD_NEXT, "read");
    if (!write_f) write_f = dlsym(RTLD_NEXT, "write");
}

// ===================== 协程上下文 =====================
ucontext_t main_ctx, co_ctx; // 主协程和其他协程

// ===================== TCP 服务器 =====================
int main() {
    init_hook();

    // --------------- 创建非阻塞 Socket ---------------
    int sockfd = socket(AF_INET, SOCK_STREAM, 0);
    fcntl(sockfd, F_SETFL, O_NONBLOCK); // 设置为非阻塞

    // --------------- 绑定端口 ---------------
    struct sockaddr_in serveraddr = {0};
    serveraddr.sin_family = AF_INET;
    serveraddr.sin_port = htons(2048);
    if (bind(sockfd, (struct sockaddr*)&serveraddr, sizeof(serveraddr)) < 0) {
        perror("bind failed");
        return -1;
    }

    // --------------- 监听连接 ---------------
    listen(sockfd, 10);
    printf("Server listening on port 2048...\n");

    // --------------- 接受连接循环 ---------------
    while (1) {
        int clientfd = accept(sockfd, NULL, NULL);
        if (clientfd < 0) {
            if (errno == EAGAIN || errno == EWOULDBLOCK) { // 非阻塞无连接
                usleep(1000); // 避免 CPU 忙等待
                continue;
            } else {
                perror("accept failed");
                break;
            }
        }

        // --------------- 处理客户端数据 ---------------
        fcntl(clientfd, F_SETFL, O_NONBLOCK); // 客户端非阻塞
        while (1) {
            char buffer[128] = {0};
            int count = read(clientfd, buffer, sizeof(buffer));
            if (count == 0) { // 客户端关闭连接
                printf("Client disconnected.\n");
                break;
            } else if (count < 0) {
                if (errno == EAGAIN) continue; // 数据未就绪
                else break;
            }

            // 回显数据
            write(clientfd, buffer, count);
        }
        close(clientfd); // 关闭客户端连接
    }

    close(sockfd); // 关闭服务器 Socket
    return 0;
}
```



#### **1. Hook 机制**

- **原理**：通过 `dlsym` 获取c原始 `read`/`write` 函数地址，定义同名函数拦截调用。

- 关键代码：

  ```c
  ssize_t read(int fd, void *buf, size_t count) {
      // 非阻塞检查数据是否就绪
      if (未就绪) swapcontext(&main_ctx, &co_ctx); // 协程切换
      return read_f(fd, buf, count); // 调用原始函数
  }
  ```

#### **2. 非阻塞 TCP 服务器**

- 步骤：
  1. **创建 Socket**：`socket(AF_INET, SOCK_STREAM, 0)`
  2. **设为非阻塞**：`fcntl(fd, F_SETFL, O_NONBLOCK)`
  3. **绑定端口**：`bind(sockfd, ...)`
  4. **监听连接**：`listen(sockfd, 10)`
  5. **Accept 循环**：非阻塞模式下处理 `EAGAIN` 错误。
  6. **处理数据**：使用 Hook 的 `read`/`write` 实现日志记录。

#### **3. 协程切换（示例）**

- **初始化上下文**：通过 `getcontext` 和 `makecontext` 初始化协程（代码中未完全实现）。
- **切换逻辑**：在 `read` 数据未就绪时，调用 `swapcontext` 切换协程。

#### **4. 错误处理**

- **Accept 错误**：处理 `EAGAIN`（非阻塞无连接）和其他错误。
- **Read 错误**：处理客户端断开 (`count == 0`) 和错误码 (`errno == EAGAIN`)。

------

#### **总结**

- **Hook 用途**：拦截系统调用，实现日志、非阻塞 I/O 或协程调度。
- **代码亮点**：非阻塞 Socket + Hook 实现高性能回显服务器。
- **扩展方向**：可结合协程库（如 libco）实现完整异步网络框架。



#### ==**如何实现「一个 fd 对应一个 ctx」：协程与 I/O 的高效协作**==

------

**核心思想**

老师的建议本质上是想通过 **协程（Coroutine）** 和 **非阻塞 I/O** 结合，实现高性能的并发模型。具体来说：

- **每个文件描述符（fd）**（如 Socket 连接）绑定一个独立的协程上下文（`ctx`）。
- 当某个 fd 的 I/O 操作（如 `read`）因数据未就绪需要等待时，**挂起当前协程**，保存其上下文（`ctx`），切换到其他就绪的协程。
- 当该 fd 的 I/O 就绪时（如数据到达），**恢复对应的协程**继续执行。

这种设计可以实现类似 `epoll` 的高效事件驱动模型，但代码风格更接近同步编程，避免回调地狱。



**1. 数据结构设计**  

需要维护一个 **全局映射表**，记录每个 fd 对应的协程上下文（`ctx`）和状态。  
```c
// 协程上下文结构体
typedef struct {
    ucontext_t ctx;    // 协程上下文
    int fd;            // 关联的文件描述符
    char buffer[128];  // 数据缓冲区（示例）
    size_t buf_size;   // 已读取数据长度
} CoroutineContext;

// 全局协程管理器（简化示例）
CoroutineContext *coroutine_table[MAX_FD]; // 哈希表或数组
```

**2. 协程初始化**  

当有新的客户端连接（`accept`）时，为每个 fd 创建协程上下文：  
```c
void create_coroutine(int fd) {
    CoroutineContext *co = malloc(sizeof(CoroutineContext));
    co->fd = fd;
    getcontext(&co->ctx); // 初始化上下文
    coroutine_table[fd] = co; // 存入映射表
    // 绑定协程入口函数（如处理客户端数据的逻辑）
    makecontext(&co->ctx, (void (*)())handle_client, 1, fd);
}
```

**3. Hook 函数改造**  

在 `read` 和 `write` 的 Hook 中，实现协程切换逻辑：  
```c
ssize_t read(int fd, void *buf, size_t count) {
    // 检查数据是否就绪
    struct pollfd fds = {.fd = fd, .events = POLLIN};
    int res = poll(&fds, 1, 0);
    
    if (res <= 0) { // 数据未就绪，挂起当前协程
        CoroutineContext *current_co = coroutine_table[fd];
        swapcontext(&current_co->ctx, &scheduler_ctx); // 切回调度器
    }
    
    // 数据已就绪，调用原始 read
    return read_f(fd, buf, count);
}
```

**4. 调度器（Scheduler）**  

一个主协程作为调度器，监听所有 fd 的事件（如 `epoll`）：  
```c
void scheduler() {
    while (1) {
        // 使用 epoll 监听所有协程的 fd
        int n = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);
        for (int i = 0; i < n; i++) {
            int fd = events[i].data.fd;
            CoroutineContext *co = coroutine_table[fd];
            swapcontext(&scheduler_ctx, &co->ctx); // 恢复协程
        }
    }
}
```

**5. 客户端处理协程**  

每个客户端连接对应一个协程，处理读写逻辑：  
```c
void handle_client(int fd) {
    while (1) {
        char buffer[128] = {0};
        int count = read(fd, buffer, sizeof(buffer));
        if (count <= 0) break;
        write(fd, buffer, count); // 回显数据
    }
    close(fd); // 关闭连接
    free(coroutine_table[fd]); // 释放协程资源
}
```

---

**流程图（Mermaid 形式）**

```mermaid
graph TD
    A[启动服务器] --> B[创建监听 Socket]
    B --> C[绑定端口并监听]
    C --> D[Accept 新连接]
    D --> |新客户端 fd| E[创建协程上下文]
    E --> F[将 fd 加入 epoll 监听]
    F --> D
    G[epoll 事件循环] --> |fd 可读| H[查找对应协程]
    H --> I[恢复协程执行]
    I --> J[处理数据并回显]
    J --> K[协程挂起等待下次 I/O]
    K --> G
```

---

**关键优势**  

1. **高效并发**：利用协程轻量级特性，单线程即可处理数万连接。  
2. **同步编程风格**：代码逻辑类似同步阻塞模型，但底层是非阻塞 I/O。  
3. **资源隔离**：每个 fd 独立协程，避免数据竞争和全局锁。  

---

**代码扩展方向**  

1. **协程池优化**：预创建协程复用资源，避免频繁创建销毁。  
2. **超时机制**：为每个协程设置超时，避免死等未响应的连接。  
3. **错误处理**：完善 `read`/`write` 的错误码处理（如 `EAGAIN`）。  







### 四、协程如何定义

用协程解决网络io处理的开源框架

基于TCP通信的协程实现详解

---

#### **4.1 协程在TCP通信中的核心作用**

TCP通信天然存在 **阻塞等待**（如`accept`、`read`）和 **高并发**（如10万连接）需求。传统多线程模型存在资源消耗大、上下文切换慢的问题。协程通过 **用户态轻量级线程** + **非阻塞I/O事件驱动** 解决此矛盾。

---

#### **4.2 协程状态机与TCP通信的映射**

图片中提到的三种状态 **`ready`、`wait`、`sleep`** 对应TCP连接的不同阶段：

| 协程状态  | TCP通信场景                                                  |
| --------- | ------------------------------------------------------------ |
| **Ready** | 连接已建立，数据可读/可写，协程处于可执行队列等待调度        |
| **Wait**  | 等待Socket数据到达（`read`阻塞）、等待发送缓冲区空闲（`write`阻塞） |
| **Sleep** | 定时任务（如心跳包超时检测、连接保活）                       |

---

#### **4.3 调度器核心结构解析**

图片中给出的调度器结构体 `struct scheduler` 是协程框架的核心：

```c
struct scheduler {
    int epfd;                          // epoll实例的文件描述符
    struct epoll_event events[1024];   // epoll事件数组
    
    // 协程状态管理数据结构
    queue_node(coroutine) ready_head;  // 就绪队列（双向链表）
    rbtree_root(coroutine) wait;        // 等待I/O的红黑树（按fd组织）
    rbtree_root(coroutine) sleep;      // 休眠协程的红黑树（按唤醒时间组织）
};
```

##### **1. `epoll`事件驱动**
- **作用**：监听所有TCP Socket的I/O事件（可读、可写、错误）。
- **关键代码**：
  ```c
  // 初始化epoll
  scheduler.epfd = epoll_create1(0);
  
  // 添加socket到epoll监听
  struct epoll_event ev;
  ev.events = EPOLLIN | EPOLLET; // 边缘触发模式
  ev.data.fd = sockfd;
  epoll_ctl(scheduler.epfd, EPOLL_CTL_ADD, sockfd, &ev);
  ```

##### **2. 就绪队列（`ready_head`）**
- **数据结构**：双向链表（`TAILQ`或手动实现）。
- **用途**：存放所有可立即执行的协程。调度器轮询此队列，依次执行协程。

##### **3. 等待红黑树（`wait`）**
- **Key**：文件描述符（`fd`）。
- **用途**：快速查找等待某个Socket I/O事件的协程。当`epoll`检测到事件时，从红黑树中找到对应协程，移入就绪队列。

##### **4. 休眠红黑树（`sleep`）**
- **Key**：唤醒时间戳（绝对时间）。
- **用途**：管理定时任务（如超时重传）。每次调度时检查最早唤醒时间，若超时则移入就绪队列。

---

#### **4.4 上下文切换：`setjmp`/`longjmp` vs `ucontext`**
图片中提到了 `setjmp` 和 `longjmp`，这是两种不同的上下文切换方式：

| 方法               | 优点                   | 缺点                       | 适用场景              |
| ------------------ | ---------------------- | -------------------------- | --------------------- |
| **setjmp/longjmp** | 轻量，不依赖系统库     | 不保存栈，需手动管理栈空间 | 简单协程或无栈协程    |
| **ucontext**       | 标准库支持，自动栈管理 | 性能略低于`setjmp`         | 复杂协程（如TCP通信） |

**TCP通信中的选择**

- **推荐 `ucontext`**：因TCP协程通常需要独立栈空间处理数据缓冲区，`ucontext`的栈管理更安全。
- **示例代码**：
  ```c
  void coroutine_entry(int fd) {
      char buffer[1024];
      while (1) {
          int n = read(fd, buffer, sizeof(buffer)); // Hook的非阻塞read
          if (n > 0) process_data(buffer);
          else if (n == 0) break; // 连接关闭
      }
  }
  ```

---

#### **4.5 处理10万并发的关键技术**
##### **1. 协程轻量级**
- **内存消耗**：每个协程独立栈（默认64KB\~1MB），10万协程仅需约6\~100GB内存（可优化至2KB/协程）。
- **切换开销**：`swapcontext` 约50ns，远低于线程切换（1~10μs）。

##### **2. 高效数据结构**
- **红黑树**：查找、插入、删除时间复杂度为O(log n)，适合管理10万级协程。
- **epoll**：内核事件通知机制，可扩展至百万连接。

##### **3. 非阻塞I/O Hook**
- **Hook系统调用**：拦截`read`、`write`等，在数据未就绪时挂起协程。
  ```c
  ssize_t read(int fd, void *buf, size_t count) {
      struct coroutine *co = current_co();
      if (数据未就绪) {
          co->state = WAIT;
          rbtree_insert(scheduler.wait, co); // 按fd插入红黑树
          swapcontext(&co->ctx, &scheduler_ctx);
      }
      return real_read(fd, buf, count);
  }
  ```

---

#### **4.6 完整TCP协程通信流程**
```mermaid
sequenceDiagram
    participant Client
    participant Server
    participant Scheduler
    participant Coroutine
    
    Client->>Server: connect()
    Server->>Scheduler: accept()触发epoll事件
    Scheduler->>Coroutine: 创建协程，加入ready队列
    Coroutine->>Scheduler: read()时数据未就绪，挂起到wait树
    Scheduler->>epoll: 监听所有fd
    epoll-->>Scheduler: 数据到达，通知对应fd
    Scheduler->>Coroutine: 从wait树移入ready队列
    Coroutine->>Client: 处理数据并回写
```

---

#### **4.7 开源框架对比**
| 框架           | 协程模型   | I/O事件管理  | 适用场景           |
| -------------- | ---------- | ------------ | ------------------ |
| **libco**      | 非对称协程 | Hook系统调用 | 微信后台高并发服务 |
| **Boost.Asio** | 基于回调   | epoll/kqueue | C++网络应用        |
| **Goroutine**  | M:N调度    | Netpoll      | Go语言生态         |

---

#### **4.8 性能优化建议**
1. **协程栈复用**：预分配栈池，避免频繁`malloc`。
2. **批量事件处理**：`epoll_wait` 一次获取多个事件，减少系统调用。
3. **零拷贝优化**：`sendfile`或`mmap`减少数据拷贝。
4. **亲和性调度**：绑定协程到特定CPU核心，利用缓存局部性。

---

#### **总结**
在TCP通信中，协程通过 **用户态调度** + **非阻塞I/O** 实现高并发。核心是通过调度器管理 `ready`、`wait`、`sleep` 状态的协程，结合 `epoll` 事件驱动和高效数据结构（红黑树），支撑10万级连接。代码需重点实现协程创建、上下文切换和I/O Hook，最终达到 **同步编码风格，异步执行性能** 的目标。





### 五、汇编实现

![image-20250331232244195](进阶学习/image-20250331232244195.png)

![image-20250331234342503](进阶学习/image-20250331234342503.png)

![image-20250331234356490](进阶学习/image-20250331234356490.png)





### 六、怎么用协程呢

![image-20250331235121821](进阶学习/image-20250331235140841.png)

![image-20250331235127221](进阶学习/image-20250331235127221.png)

![image-20250331235311323](进阶学习/image-20250331235311323.png)

![image-20250331235316524](进阶学习/image-20250331235316524.png)

![image-20250331235328226](进阶学习/image-20250331235328226.png)

![image-20250331235333151](进阶学习/image-20250331235333151.png)
